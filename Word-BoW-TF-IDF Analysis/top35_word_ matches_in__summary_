{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1LJtkO2L7-0wp3HZUFQRTyYFsuQnL4VK5","timestamp":1751567162879}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oEU1G267dRJ-","executionInfo":{"status":"ok","timestamp":1751570310971,"user_tz":300,"elapsed":455,"user":{"displayName":"Nicole Page","userId":"05844542880347904373"}},"outputId":"270d890a-9b06-42bd-f834-0ed5af760b7b"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["# load packages\n","import numpy as np\n","import pandas as pd\n","import nltk\n","nltk.download('stopwords')\n","from nltk.corpus import stopwords\n","import string\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","import plotly.express as px\n","from wordcloud import WordCloud\n","import matplotlib.pyplot as plt\n","#% matplotlib inline"],"metadata":{"id":"qj_liV4ZadVh","executionInfo":{"status":"ok","timestamp":1751570315649,"user_tz":300,"elapsed":4683,"user":{"displayName":"Nicole Page","userId":"05844542880347904373"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"c0155a8e-b28e-42a8-9788-5a2a717b0494"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]}]},{"cell_type":"code","source":["df= pd.read_csv('/content/gdrive/MyDrive/palm_oil_grievance_logs.csv')"],"metadata":{"id":"EImuREcMeOYX","executionInfo":{"status":"ok","timestamp":1751570315745,"user_tz":300,"elapsed":95,"user":{"displayName":"Nicole Page","userId":"05844542880347904373"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["#stop words, lower case, and remove punctuation\n","stop_words = set(stopwords.words('english'))\n","punctuation = set(string.punctuation)\n","\n","# preprocessing function\n","def clean_text(text):\n","    # convert non-string types to string for potential na vals\n","    if not isinstance(text, str):\n","        text = str(text)\n","\n","    text = text.lower()\n","    tokens = text.split()\n","    tokens = [word.strip(\"\".join(punctuation)) for word in tokens]\n","    tokens = [word for word in tokens if word and word not in stop_words]\n","    return tokens\n","\n","# Add cleaned tokens to the df\n","df[\"tokens\"] = df[\"summary\"].apply(clean_text)\n","\n","# Define the list of words to filter out\n","words_to_filter = {'nan'}\n","\n","# filter out words that we think are not helpful\n","df['flt_tokens'] = df['tokens'].apply(lambda tokens: [word for word in tokens if word not in words_to_filter])\n","\n","# use filtered tokens and join back to string\n","textdf_clean = df['flt_tokens'].apply(lambda x: ' '.join(x))"],"metadata":{"id":"a_nxBLfhepun","executionInfo":{"status":"ok","timestamp":1751570315936,"user_tz":300,"elapsed":188,"user":{"displayName":"Nicole Page","userId":"05844542880347904373"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"8919c2c3","executionInfo":{"status":"ok","timestamp":1751570316059,"user_tz":300,"elapsed":125,"user":{"displayName":"Nicole Page","userId":"05844542880347904373"}}},"source":["# try bag of words (bow)\n","\n","count_vectorizer = CountVectorizer()\n","X_bow = count_vectorizer.fit_transform(textdf_clean)\n","\n","# bow dataframe\n","bow_df = pd.DataFrame(X_bow.toarray(), columns=count_vectorizer.get_feature_names_out())"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"9f26a9c8","executionInfo":{"status":"ok","timestamp":1751570316060,"user_tz":300,"elapsed":5,"user":{"displayName":"Nicole Page","userId":"05844542880347904373"}}},"source":["# add up word counts\n","word_counts = bow_df.sum(axis=0)\n","\n","# sort the words by their total counts (descending)\n","bowtop_words = word_counts.sort_values(ascending=False)\n","\n","# top 20 most frequent words\n","num_words = 50 # can change # to have more or less words"],"execution_count":6,"outputs":[]},{"cell_type":"code","source":["#try TF-IDF\n","# convert text to TF-IDF\n","vectorizer = TfidfVectorizer(stop_words='english')\n","# try wordnet visualization\n","\n","X_tfidf = vectorizer.fit_transform(textdf_clean).toarray()\n","\n","# scores\n","term_frequencies = np.array(X_tfidf > 0, dtype=int).sum(axis=0)\n","n_documents = len(textdf_clean)\n","idf_scores = np.log(n_documents / (term_frequencies + 1))\n","\n","# Compute manual TF-IDF matrix\n","manual_tfidf = X_tfidf * idf_scores\n","\n","manual_tfidf_df = pd.DataFrame(manual_tfidf, columns=vectorizer.get_feature_names_out())"],"metadata":{"id":"TuFdd9MVb-zY","executionInfo":{"status":"ok","timestamp":1751570316188,"user_tz":300,"elapsed":131,"user":{"displayName":"Nicole Page","userId":"05844542880347904373"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["# sum the TF-IDF scores for each word\n","word_scores = manual_tfidf_df.sum(axis=0)\n","\n","# Sort the words by their total TF-IDF scores in descending order\n","tftop_words = word_scores.sort_values(ascending=False)"],"metadata":{"id":"fpXbtj4Qk7c8","executionInfo":{"status":"ok","timestamp":1751570316189,"user_tz":300,"elapsed":11,"user":{"displayName":"Nicole Page","userId":"05844542880347904373"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"29e85b76","executionInfo":{"status":"ok","timestamp":1751570316189,"user_tz":300,"elapsed":9,"user":{"displayName":"Nicole Page","userId":"05844542880347904373"}},"outputId":"33d9226c-74be-4970-a57f-473b28c494e3"},"source":["# dataframe of both outputs\n","bow_df_sorted = bowtop_words.reset_index()\n","bow_df_sorted.columns = ['BOW_Word', 'BOW_Frequency']\n","\n","tfidf_df_sorted = tftop_words.reset_index()\n","tfidf_df_sorted.columns = ['TFIDF_Word', 'TFIDF_Score']\n","\n","\n","combined_word_analysis_df = pd.concat([bow_df_sorted, tfidf_df_sorted], axis=1)\n","\n","print(combined_word_analysis_df)"],"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["     BOW_Word  BOW_Frequency  TFIDF_Word  TFIDF_Score\n","0          pt            490  respondent    26.061035\n","1      report            207          pt    23.230367\n","2        palm            189     workers    18.382567\n","3        land            178        land    18.129738\n","4         oil            177     company    17.912401\n","...       ...            ...         ...          ...\n","4002   marita              1         NaN          NaN\n","4003   marine              1         NaN          NaN\n","4004     mara              1         NaN          NaN\n","4005    close              1         NaN          NaN\n","4006  mandate              1         NaN          NaN\n","\n","[4007 rows x 4 columns]\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","import re\n","\n","# ------------------------------------------------------------------\n","# 0.  READ YOUR DATA  ------------------------------------------------\n","# ------------------------------------------------------------------\n","# Grievance log that has at least:  pk  |  summary\n","df = pd.read_csv(\"/content/gdrive/MyDrive/palm_oil_grievance_logs.csv\")\n","\n","# If you have *already* built this, skip.  Otherwise build / load it:\n","#    combined_word_analysis_df  with columns:\n","#    BOW_Word  |  BOW_Frequency  |  TFIDF_Word  |  TFIDF_Score\n","# (For example, see your earlier notebook steps.)\n","# ------------------------------------------------------------------\n","\n","\n","# ------------------------------------------------------------------\n","# 1.  GET THE TOP-35 WORD LISTS  ------------------------------------\n","# ------------------------------------------------------------------\n","top_tfidf_words = (combined_word_analysis_df\n","                   .sort_values(\"TFIDF_Score\", ascending=False)\n","                   [\"TFIDF_Word\"]\n","                   .head(35)\n","                   .tolist())\n","\n","top_bow_words   = (combined_word_analysis_df\n","                   .sort_values(\"BOW_Frequency\", ascending=False)\n","                   [\"BOW_Word\"]\n","                   .head(35)\n","                   .tolist())\n","\n","\n","# ------------------------------------------------------------------\n","# 2.  HELPER FUNCTIONS  --------------------------------------------\n","# ------------------------------------------------------------------\n","def extract_matching_rows(df, word_list,\n","                          pk_column=\"pk\",\n","                          text_column=\"summary\"):\n","    \"\"\"\n","    Return a DataFrame with one row per (Word, pk) pair\n","    → no duplicate rows even if the word appears many times.\n","    \"\"\"\n","    frames = []\n","    for word in word_list:\n","        pattern = fr\"\\b{re.escape(word)}\\b\"\n","        mask = df[text_column].str.contains(pattern,\n","                                            case=False,\n","                                            na=False,\n","                                            regex=True)\n","        sub = df.loc[mask, [pk_column, text_column]].copy()\n","        sub[\"Word\"] = word\n","        frames.append(sub[[\"Word\", pk_column, text_column]])\n","\n","    out = pd.concat(frames, ignore_index=True)\n","    out = out.drop_duplicates(subset=[\"Word\", pk_column])   # de-duplicate\n","    return out\n","\n","\n","def sort_matches_by_word_order(matches_df, ranked_words):\n","    \"\"\"Preserve the original ranking order of the word list.\"\"\"\n","    order_map = {w: i for i, w in enumerate(ranked_words)}\n","    return (matches_df\n","            .assign(_order=matches_df[\"Word\"].map(order_map))\n","            .sort_values(\"_order\")\n","            .drop(columns=\"_order\"))\n","\n","\n","# ------------------------------------------------------------------\n","# 3.  EXTRACT → SORT → SAVE  ---------------------------------------\n","# ------------------------------------------------------------------\n","# TF-IDF\n","tfidf_matches      = extract_matching_rows(df, top_tfidf_words)\n","tfidf_sorted       = sort_matches_by_word_order(tfidf_matches, top_tfidf_words)\n","tfidf_sorted.to_csv(\"tfidf_word_matches.csv\", index=False)\n","\n","# BoW\n","bow_matches        = extract_matching_rows(df, top_bow_words)\n","bow_sorted         = sort_matches_by_word_order(bow_matches, top_bow_words)\n","bow_sorted.to_csv(\"bow_word_matches.csv\",  index=False)\n","\n","print(\"Created tfidf_word_matches.csv and bow_word_matches.csv\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Jks14zRgtRdn","executionInfo":{"status":"ok","timestamp":1751570317358,"user_tz":300,"elapsed":1171,"user":{"displayName":"Nicole Page","userId":"05844542880347904373"}},"outputId":"61b7ff80-c4c4-4a52-db28-ef29e9be98d9"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Created tfidf_word_matches.csv and bow_word_matches.csv\n"]}]}]}