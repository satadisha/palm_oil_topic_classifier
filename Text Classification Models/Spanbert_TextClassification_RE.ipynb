{"cells":[{"cell_type":"markdown","metadata":{"id":"lahf6KvZympt"},"source":["[link text](https://)# Pre-Trained Word Embeddings for Text Classification"]},{"cell_type":"markdown","metadata":{"id":"ccOyI07MyolN"},"source":["# Imports/Installations"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":13652,"status":"ok","timestamp":1754341411829,"user":{"displayName":"Owakamare Princewill","userId":"11234081405779952785"},"user_tz":300},"id":"KHPPdnj2Wzz5","outputId":"c0cefb51-4071-4e48-87d5-07af9974ea49"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.54.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.34.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n","Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.14.1)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.5)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.7.14)\n","Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (4.0.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n","Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n","Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n","Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n","Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n","Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n","Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n","Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.34.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (25.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n","Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.14)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.14.1)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.1.5)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.7.14)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n","Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.3)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n","Requirement already satisfied: setfit in /usr/local/lib/python3.11/dist-packages (1.1.2)\n","Requirement already satisfied: datasets>=2.15.0 in /usr/local/lib/python3.11/dist-packages (from setfit) (4.0.0)\n","Requirement already satisfied: sentence-transformers>=3 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers[train]>=3->setfit) (4.1.0)\n","Requirement already satisfied: transformers>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from setfit) (4.54.0)\n","Requirement already satisfied: evaluate>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from setfit) (0.4.5)\n","Requirement already satisfied: huggingface_hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from setfit) (0.34.1)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from setfit) (1.6.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from setfit) (25.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.15.0->setfit) (3.18.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.15.0->setfit) (2.0.2)\n","Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.15.0->setfit) (18.1.0)\n","Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.15.0->setfit) (0.3.8)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets>=2.15.0->setfit) (2.2.2)\n","Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.15.0->setfit) (2.32.3)\n","Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.15.0->setfit) (4.67.1)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets>=2.15.0->setfit) (3.5.0)\n","Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.15.0->setfit) (0.70.16)\n","Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.15.0->setfit) (2025.3.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.15.0->setfit) (6.0.2)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.24.0->setfit) (4.14.1)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.24.0->setfit) (1.1.5)\n","Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=3->sentence-transformers[train]>=3->setfit) (2.6.0+cu124)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=3->sentence-transformers[train]>=3->setfit) (1.16.0)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=3->sentence-transformers[train]>=3->setfit) (11.3.0)\n","Requirement already satisfied: accelerate>=0.20.3 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers[train]>=3->setfit) (1.9.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.41.0->setfit) (2024.11.6)\n","Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.41.0->setfit) (0.21.2)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.41.0->setfit) (0.5.3)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->setfit) (1.5.1)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->setfit) (3.6.0)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.20.3->sentence-transformers[train]>=3->setfit) (5.9.5)\n","Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.15.0->setfit) (3.12.14)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.15.0->setfit) (3.4.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.15.0->setfit) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.15.0->setfit) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.15.0->setfit) (2025.7.14)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=3->sentence-transformers[train]>=3->setfit) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=3->sentence-transformers[train]>=3->setfit) (3.1.6)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=3->sentence-transformers[train]>=3->setfit) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=3->sentence-transformers[train]>=3->setfit) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=3->sentence-transformers[train]>=3->setfit) (12.4.127)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=3->sentence-transformers[train]>=3->setfit) (9.1.0.70)\n","Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=3->sentence-transformers[train]>=3->setfit) (12.4.5.8)\n","Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=3->sentence-transformers[train]>=3->setfit) (11.2.1.3)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=3->sentence-transformers[train]>=3->setfit) (10.3.5.147)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=3->sentence-transformers[train]>=3->setfit) (11.6.1.9)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=3->sentence-transformers[train]>=3->setfit) (12.3.1.170)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=3->sentence-transformers[train]>=3->setfit) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=3->sentence-transformers[train]>=3->setfit) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=3->sentence-transformers[train]>=3->setfit) (12.4.127)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=3->sentence-transformers[train]>=3->setfit) (12.4.127)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=3->sentence-transformers[train]>=3->setfit) (3.2.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=3->sentence-transformers[train]>=3->setfit) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers>=3->sentence-transformers[train]>=3->setfit) (1.3.0)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=2.15.0->setfit) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=2.15.0->setfit) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=2.15.0->setfit) (2025.2)\n","Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.15.0->setfit) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.15.0->setfit) (1.4.0)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.15.0->setfit) (25.3.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.15.0->setfit) (1.7.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.15.0->setfit) (6.6.3)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.15.0->setfit) (0.3.2)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.15.0->setfit) (1.20.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.15.0->setfit) (1.17.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers>=3->sentence-transformers[train]>=3->setfit) (3.0.2)\n"]}],"source":["!pip install transformers\n","!pip install -U datasets\n","!pip install setfit\n","\n","from datasets import load_dataset\n","from sentence_transformers.losses import CosineSimilarityLoss\n","\n","from setfit import SetFitModel, SetFitTrainer"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":638,"status":"ok","timestamp":1754341412506,"user":{"displayName":"Owakamare Princewill","userId":"11234081405779952785"},"user_tz":300},"id":"xO5E_RiCW8RV","outputId":"82ad52bd-a110-4ef3-c6db-0d1c090a19df"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":22,"status":"ok","timestamp":1754341412534,"user":{"displayName":"Owakamare Princewill","userId":"11234081405779952785"},"user_tz":300},"id":"RjMDjd7tsgf7"},"outputs":[],"source":["from collections import defaultdict, Counter\n","import json\n","import numpy as np\n","import torch\n","import pandas as pd\n","import random\n","random_seed = 10\n","random.seed(random_seed)\n","\n","\n","from matplotlib import pyplot as plt\n","\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoModel\n","from datasets import load_dataset, DatasetDict\n","from torch.utils.data import DataLoader\n","from transformers import DistilBertConfig, DistilBertTokenizer, DistilBertForSequenceClassification, DistilBertModel\n","from transformers import get_linear_schedule_with_warmup\n","from tqdm.notebook import tqdm\n","from torch.optim import AdamW\n","from transformers import set_seed\n","from sklearn.metrics import accuracy_score, f1_score\n","from datasets import load_dataset, DatasetDict, Dataset\n","from torch.utils.data import DataLoader"]},{"cell_type":"markdown","source":["## New Way of Choosing Data"],"metadata":{"id":"BwfxhV7-phI3"}},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","dominant_topic_df = pd.read_csv('/content/gdrive/MyDrive/Group 3: palm oil topic classifier/Data/Labeled Data/dominant_topic_results.csv')\n","\n","# Assuming dominant_topic_df is the DataFrame you want to split\n","# Define the features (X) and the target (y)\n","# Here, we'll use the 'Text' column as the feature\n","X = dominant_topic_df['Text']\n","\n","# And the topic probability columns and Dominant_Topic as the target (y)\n","# Changed to include Topic_0 through Topic_5 instead of the percentage columns\n","topic_columns = [col for col in dominant_topic_df.columns if col.startswith('Topic_') and len(col) == 7 and not col.endswith('_Perc')]\n","y = dominant_topic_df[['Document_Num'] + topic_columns]\n","\n","\n","# Perform the 80/20 split\n","X_train, X_val, y_train, y_val = train_test_split(\n","    X, y,\n","    test_size=0.2,  # 20% for validation\n","    random_state=random_seed # Use the predefined random_seed for reproducibility\n","    # Stratify is not directly applicable to multi-output targets like this,\n","    # so we will remove it.\n",")\n","\n","print(\"Training set size:\", len(X_train))\n","print(\"Validation set size:\", len(X_val))\n","print(\"\\nShape of X_train:\", X_train.shape)\n","print(\"Shape of X_val:\", X_val.shape)\n","print(\"Shape of y_train:\", y_train.shape)\n","print(\"Shape of y_val:\", y_val.shape)\n","\n","y"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":546},"id":"er0g6jjopkaM","executionInfo":{"status":"ok","timestamp":1754341412676,"user_tz":300,"elapsed":140,"user":{"displayName":"Owakamare Princewill","userId":"11234081405779952785"}},"outputId":"84bee76d-a0a7-4339-95c9-41b2e6f920c3"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Training set size: 309\n","Validation set size: 78\n","\n","Shape of X_train: (309,)\n","Shape of X_val: (78,)\n","Shape of y_train: (309, 7)\n","Shape of y_val: (78, 7)\n"]},{"output_type":"execute_result","data":{"text/plain":["     Document_Num  Topic_0  Topic_1  Topic_2  Topic_3  Topic_4  Topic_5\n","0               0        0        0        0        0        1        0\n","1               1        0        0        0        0        1        0\n","2               2        1        0        0        0        0        0\n","3               3        0        0        1        0        0        0\n","4               4        0        0        0        0        0        1\n","..            ...      ...      ...      ...      ...      ...      ...\n","382           382        1        0        0        0        0        0\n","383           383        0        0        1        0        0        1\n","384           384        0        1        0        0        0        0\n","385           385        0        1        0        1        0        0\n","386           386        0        1        0        1        0        0\n","\n","[387 rows x 7 columns]"],"text/html":["\n","  <div id=\"df-62d1b870-1f62-4a00-b5d6-c99dd28f4daa\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Document_Num</th>\n","      <th>Topic_0</th>\n","      <th>Topic_1</th>\n","      <th>Topic_2</th>\n","      <th>Topic_3</th>\n","      <th>Topic_4</th>\n","      <th>Topic_5</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>382</th>\n","      <td>382</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>383</th>\n","      <td>383</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>384</th>\n","      <td>384</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>385</th>\n","      <td>385</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>386</th>\n","      <td>386</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>387 rows × 7 columns</p>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-62d1b870-1f62-4a00-b5d6-c99dd28f4daa')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-62d1b870-1f62-4a00-b5d6-c99dd28f4daa button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-62d1b870-1f62-4a00-b5d6-c99dd28f4daa');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","    <div id=\"df-bc61673e-5b47-43cb-8cd2-8d2f07a5fce8\">\n","      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-bc61673e-5b47-43cb-8cd2-8d2f07a5fce8')\"\n","                title=\"Suggest charts\"\n","                style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","      </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","      <script>\n","        async function quickchart(key) {\n","          const quickchartButtonEl =\n","            document.querySelector('#' + key + ' button');\n","          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","          quickchartButtonEl.classList.add('colab-df-spinner');\n","          try {\n","            const charts = await google.colab.kernel.invokeFunction(\n","                'suggestCharts', [key], {});\n","          } catch (error) {\n","            console.error('Error during call to suggestCharts:', error);\n","          }\n","          quickchartButtonEl.classList.remove('colab-df-spinner');\n","          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","        }\n","        (() => {\n","          let quickchartButtonEl =\n","            document.querySelector('#df-bc61673e-5b47-43cb-8cd2-8d2f07a5fce8 button');\n","          quickchartButtonEl.style.display =\n","            google.colab.kernel.accessAllowed ? 'block' : 'none';\n","        })();\n","      </script>\n","    </div>\n","\n","  <div id=\"id_7ce9b0a5-cd1d-4ebd-8acc-09681be25c99\">\n","    <style>\n","      .colab-df-generate {\n","        background-color: #E8F0FE;\n","        border: none;\n","        border-radius: 50%;\n","        cursor: pointer;\n","        display: none;\n","        fill: #1967D2;\n","        height: 32px;\n","        padding: 0 0 0 0;\n","        width: 32px;\n","      }\n","\n","      .colab-df-generate:hover {\n","        background-color: #E2EBFA;\n","        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","        fill: #174EA6;\n","      }\n","\n","      [theme=dark] .colab-df-generate {\n","        background-color: #3B4455;\n","        fill: #D2E3FC;\n","      }\n","\n","      [theme=dark] .colab-df-generate:hover {\n","        background-color: #434B5C;\n","        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","        fill: #FFFFFF;\n","      }\n","    </style>\n","    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('y')\"\n","            title=\"Generate code using this dataframe.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n","  </svg>\n","    </button>\n","    <script>\n","      (() => {\n","      const buttonEl =\n","        document.querySelector('#id_7ce9b0a5-cd1d-4ebd-8acc-09681be25c99 button.colab-df-generate');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      buttonEl.onclick = () => {\n","        google.colab.notebook.generateWithVariable('y');\n","      }\n","      })();\n","    </script>\n","  </div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"y","summary":"{\n  \"name\": \"y\",\n  \"rows\": 387,\n  \"fields\": [\n    {\n      \"column\": \"Document_Num\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 111,\n        \"min\": 0,\n        \"max\": 386,\n        \"num_unique_values\": 387,\n        \"samples\": [\n          314,\n          152,\n          90\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Topic_0\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Topic_1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Topic_2\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Topic_3\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Topic_4\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Topic_5\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":11}]},{"cell_type":"markdown","metadata":{"id":"9MqIs-u9zELg"},"source":["# Loading and formatting CSVs"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":31,"status":"ok","timestamp":1754341412711,"user":{"displayName":"Owakamare Princewill","userId":"11234081405779952785"},"user_tz":300},"id":"8v2Iw3AJy0us"},"outputs":[],"source":["# df = pd.read_csv('/content/gdrive/MyDrive/Group 3: palm oil topic classifier/Data/Labeled Data/BERTopic_results.csv')\n","# test = pd.read_csv('/content/gdrive/MyDrive/Group 3: palm oil topic classifier/Data/Labeled Data/BERTopic_TEST.csv')\n","# train = pd.read_csv('/content/gdrive/MyDrive/Group 3: palm oil topic classifier/Data/Labeled Data/BERTopic_TRAIN.csv')\n","# validation = pd.read_csv('/content/gdrive/MyDrive/Group 3: palm oil topic classifier/Data/Labeled Data/BERTopic_VAL.csv')\n","\n","# # turning into hugging face format\n","# test = Dataset.from_pandas(test)\n","# train = Dataset.from_pandas(train)\n","# validation = Dataset.from_pandas(validation)"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":21,"status":"ok","timestamp":1754341412736,"user":{"displayName":"Owakamare Princewill","userId":"11234081405779952785"},"user_tz":300},"id":"7kdh1W34Hic8"},"outputs":[],"source":["# df = pd.read_csv('/content/gdrive/MyDrive/Group 3: palm oil topic classifier/Data/Labeled Data/BERTopic_results.csv')\n","# test = pd.read_csv('/content/gdrive/MyDrive/Group 3: palm oil topic classifier/Data/Labeled Data/BERTopic_TEST.csv')\n","# train = pd.read_csv('/content/gdrive/MyDrive/Group 3: palm oil topic classifier/Text Classification Models/Classification Data/Base/BERTopic_TRAIN_80.csv')\n","# validation = pd.read_csv('/content/gdrive/MyDrive/Group 3: palm oil topic classifier/Text Classification Models/Classification Data/Base/BERTopic_VAL_20.csv')\n","# # dominant_topic_df = pd.read_csv('/content/gdrive/MyDrive/Group 3: palm oil topic classifier/Data/Labeled Data/dominant_topic_results.csv')\n","\n","# # turning into hugging face format\n","# test = Dataset.from_pandas(test)\n","# train = Dataset.from_pandas(train)\n","# validation = Dataset.from_pandas(validation)"]},{"cell_type":"code","source":["test_df = pd.read_csv('/content/gdrive/MyDrive/Group 3: palm oil topic classifier/Text Classification Models/gbt_classified_grievances')\n","\n","# test_df = test_df[['pk', 'summary', 'GPT_Topic_0', 'GPT_Topic_1', 'GPT_Topic_2', 'GPT_Topic_3', 'GPT_Topic_4', \"GPT_Topic_5\"]]\n","test_df = test_df[['pk', 'summary', 'Topic_0', 'Topic_1', 'Topic_2', 'Topic_3', 'Topic_4', \"Topic_5\"]]\n","\n","display(test_df.head())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":356},"id":"NeoinQD8qGzQ","executionInfo":{"status":"error","timestamp":1754341412807,"user_tz":300,"elapsed":68,"user":{"displayName":"Owakamare Princewill","userId":"11234081405779952785"}},"outputId":"468e8dd1-52c7-4f9b-db36-4b74f8d429c4"},"execution_count":14,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/content/gdrive/MyDrive/Group 3: palm oil topic classifier/Text Classification Models/gbt_classified_grievances'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-4175350111.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/gdrive/MyDrive/Group 3: palm oil topic classifier/Text Classification Models/gbt_classified_grievances'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# test_df = test_df[['pk', 'summary', 'GPT_Topic_0', 'GPT_Topic_1', 'GPT_Topic_2', 'GPT_Topic_3', 'GPT_Topic_4', \"GPT_Topic_5\"]]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtest_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pk'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'summary'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Topic_0'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Topic_1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Topic_2'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Topic_3'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Topic_4'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Topic_5\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/gdrive/MyDrive/Group 3: palm oil topic classifier/Text Classification Models/gbt_classified_grievances'"]}]},{"cell_type":"code","source":["# word_counts = df['Text'].astype(str).apply(lambda x: len(x.split()))\n","\n","# max_words = word_counts.max()\n","# min_words = word_counts.min()\n","# mean_words = word_counts.mean()\n","\n","# print(f\"Max words in summary: {max_words}\")\n","# print(f\"Min words in summary: {min_words}\")\n","# print(f\"Mean words in summary: {mean_words}\")\n"],"metadata":{"id":"3vMcbmQqJMY1","executionInfo":{"status":"aborted","timestamp":1754341412830,"user_tz":300,"elapsed":14854,"user":{"displayName":"Owakamare Princewill","userId":"11234081405779952785"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oZtG8HV3zRWg"},"source":["# Dataset Tokenization"]},{"cell_type":"markdown","metadata":{"id":"z0y1z2eezZmz"},"source":["## Example Tokenization"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":2,"status":"aborted","timestamp":1754341412835,"user":{"displayName":"Owakamare Princewill","userId":"11234081405779952785"},"user_tz":300},"id":"B-QnaAguTQEt"},"outputs":[],"source":["name = \"SpanBERT/spanbert-large-cased\"\n","tokenizer = AutoTokenizer.from_pretrained(name)\n","\n","sample_input = \"We want to use a pretrained tokenizer.\"\n","tokenized_inputs = tokenizer(sample_input,\n","                             return_tensors=\"pt\",\n","                             padding=True,\n","                             truncation=True,\n","                             max_length=512)\n","print(tokenized_inputs[\"input_ids\"])"]},{"cell_type":"markdown","metadata":{"id":"e1E0GoSYzhpM"},"source":["We will use the function that we use to test the tokenizer on a single input."]},{"cell_type":"markdown","metadata":{"id":"kc5-wLu3z8Px"},"source":["## Tokenized Train Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1,"status":"aborted","timestamp":1754341412839,"user":{"displayName":"Owakamare Princewill","userId":"11234081405779952785"},"user_tz":300},"id":"RQijzLU729kG"},"outputs":[],"source":["tokenizer_length = 512"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":14873,"status":"aborted","timestamp":1754341412852,"user":{"displayName":"Owakamare Princewill","userId":"11234081405779952785"},"user_tz":300},"id":"kfQETJGOzh05"},"outputs":[],"source":["# tokenizer = AutoTokenizer.from_pretrained(\"SpanBERT/spanbert-large-cased\")\n","\n","# # Apply tokenization using map\n","# tokenized_train = train.map(\n","#     lambda example: tokenizer(example['Text'],\n","#                              padding=\"max_length\",\n","#                              truncation=True,\n","#                              max_length=tokenizer_length)  # not sure what length to use\n","# )\n","\n","# # Remove the original text column (we don't need it after tokenization)\n","# tokenized_train = tokenized_train.remove_columns(['Text'])\n","\n","# # Rename 'Dominant_Topic' to 'labels' (standard for transformers)\n","# tokenized_train = tokenized_train.rename_column(\"Dominant_Topic\", \"labels\")\n","\n","# # Step 4: Set format to torch tensors\n","# tokenized_train.set_format(\"torch\")\n","\n","# # Check the results\n","# print(\"Tokenized dataset features:\", tokenized_train.column_names)\n","# print(\"Dataset size:\", len(tokenized_train))\n","# print(\"\\nSample data shapes:\")\n","# #print(f\"- input_ids: {tokenized_train[0]['input_ids'].shape}\")\n","# print(f\"- attention_mask: {tokenized_train[0]['attention_mask'].shape}\")\n","# print(f\"- labels: {tokenized_train[0]['labels']}\")\n","# print(f\"- labels type: {type(tokenized_train[0]['labels'])}\")"]},{"cell_type":"code","source":["topic_columns = [col for col in dominant_topic_df.columns if col.startswith('Topic_') and len(col) == 7 and not col.endswith('_Perc')]\n","topic_columns_test = [col for col in test_df.columns if col.startswith('GPT_Topic_') and len(col) == 10 and not col.endswith('_Perc')]\n","train_dataset = Dataset.from_pandas(pd.DataFrame({'Text': X_train, 'labels': y_train[topic_columns].values.tolist()}))\n","val_dataset = Dataset.from_pandas(pd.DataFrame({'Text': X_val, 'labels': y_val[topic_columns].values.tolist()}))\n","\n","# Apply tokenization using map\n","tokenized_train = train_dataset.map(\n","    lambda example: tokenizer(example['Text'],\n","                             padding=\"max_length\",\n","                             truncation=True,\n","                             max_length=tokenizer_length),\n","    batched=True\n",")\n","\n","# Remove the original text column (we don't need it after tokenization)\n","tokenized_train = tokenized_train.remove_columns(['Text'])\n","\n","# Set format to torch tensors\n","tokenized_train.set_format(\"torch\")\n","\n","# Check the results\n","print(\"Tokenized training dataset features:\", tokenized_train.column_names)\n","print(\"Training dataset size:\", len(tokenized_train))\n","print(\"\\nSample training data shapes:\")\n","print(f\"- input_ids: {tokenized_train[0]['input_ids'].shape}\")\n","print(f\"- attention_mask: {tokenized_train[0]['attention_mask'].shape}\")\n","print(f\"- labels shape: {tokenized_train[0]['labels'].shape}\")\n","print(f\"- labels type: {type(tokenized_train[0]['labels'])}\")\n","\n","\n","tokenized_validation = val_dataset.map(\n","    lambda example: tokenizer(example['Text'],\n","                             padding=\"max_length\",\n","                             truncation=True,\n","                             max_length=tokenizer_length,),\n","    batched=True\n",")\n","\n","tokenized_validation = tokenized_validation.remove_columns(['Text'])\n","\n","tokenized_validation.set_format(\"torch\")\n","\n","print(\"\\nTokenized validation dataset size:\", len(tokenized_validation))\n","print(\"Sample validation data shapes:\")\n","print(f\"- input_ids: {tokenized_validation[0]['input_ids'].shape}\")\n","print(f\"- attention_mask: {tokenized_validation[0]['attention_mask'].shape}\")\n","print(f\"- labels shape: {tokenized_validation[0]['labels'].shape}\")\n","print(f\"- labels type: {type(tokenized_validation[0]['labels'])}\")\n","\n","\n","# Test\n","# Separate features and labels for the test set\n","# Use 'summary' as the text feature and the multi-hot encoded topics as the labels\n","X_test = test_df['summary']\n","# Convert the multi-hot encoded topic labels to a list of lists\n","y_test_labels_list = test_df[topic_columns_test].values.tolist()\n","test_pk = test_df['pk'] # Keep 'pk' separately\n","\n","test_dataset = Dataset.from_pandas(pd.DataFrame({'Text': X_test, 'labels': y_test_labels_list, 'pk': test_pk}))\n","\n","tokenized_test = test_dataset.map(\n","     lambda example: tokenizer(example['Text'],\n","                              padding=\"max_length\",\n","                              truncation=True,\n","                              max_length=tokenizer_length),\n","    batched=True\n",")\n","\n","tokenized_test = tokenized_test.remove_columns(['Text'])\n","tokenized_test.set_format(\"torch\")\n","\n","print(\"\\nTokenized test dataset size:\", len(tokenized_test))\n","print(\"Sample test data shapes:\")\n","print(f\"- input_ids: {tokenized_test[0]['input_ids'].shape}\")\n","print(f\"- attention_mask: {tokenized_test[0]['attention_mask'].shape}\")\n","# The labels for the test set are now the multi-hot encoded labels\n","print(f\"- labels: {tokenized_test[0]['labels']}\")\n","print(f\"- labels type: {type(tokenized_test[0]['labels'])}\")\n","print(f\"- pk: {tokenized_test[0]['pk']}\")\n","print(f\"- pk type: {type(tokenized_test[0]['pk'])}\")"],"metadata":{"id":"9dmHsRtTtbiG","executionInfo":{"status":"aborted","timestamp":1754341412854,"user_tz":300,"elapsed":14874,"user":{"displayName":"Owakamare Princewill","userId":"11234081405779952785"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7nc1jlHr0BTg"},"source":["## Tokenized Test Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":14875,"status":"aborted","timestamp":1754341412856,"user":{"displayName":"Owakamare Princewill","userId":"11234081405779952785"},"user_tz":300},"id":"hit2Ksg-0A2L"},"outputs":[],"source":["# # Apply the same process to your test set\n","# tokenized_test = test.map(\n","#     lambda example: tokenizer(example['Text'],\n","#                              padding=\"max_length\",\n","#                              truncation=True,\n","#                              max_length=tokenizer_length)\n","# )\n","\n","# tokenized_test = tokenized_test.remove_columns(['Text'])\n","# tokenized_test = tokenized_test.rename_column(\"Dominant_Topic\", \"labels\")\n","# tokenized_test.set_format(\"torch\")\n","\n","# print(\"Test dataset size:\", len(tokenized_test))"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":14876,"status":"aborted","timestamp":1754341412857,"user":{"displayName":"Owakamare Princewill","userId":"11234081405779952785"},"user_tz":300},"id":"-5zGmNQTUjsK"},"outputs":[],"source":["# tokenized_test['labels']"]},{"cell_type":"code","source":["tokenized_train['labels'][5]"],"metadata":{"id":"dlW_SOwAJhkF","executionInfo":{"status":"aborted","timestamp":1754341412872,"user_tz":300,"elapsed":14890,"user":{"displayName":"Owakamare Princewill","userId":"11234081405779952785"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yEDzcfm40IDD"},"source":["## Tokenized Validation Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":14890,"status":"aborted","timestamp":1754341412873,"user":{"displayName":"Owakamare Princewill","userId":"11234081405779952785"},"user_tz":300},"id":"SdFbKJ060Kyo"},"outputs":[],"source":["# Apply the same process to your test set\n","tokenized_validation = validation.map(\n","    lambda example: tokenizer(example['Text'],\n","                             padding=\"max_length\",\n","                             truncation=True,\n","                             max_length=tokenizer_length,)\n",")\n","\n","tokenized_validation = tokenized_validation.remove_columns(['Text'])\n","tokenized_validation = tokenized_validation.rename_column(\"Dominant_Topic\", \"labels\")\n","tokenized_validation.set_format(\"torch\")\n","\n","print(\"Test dataset size:\", len(tokenized_validation))"]},{"cell_type":"markdown","metadata":{"id":"Jz58ylJW0R42"},"source":["## Check Tokenization on train and test"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":14891,"status":"aborted","timestamp":1754341412874,"user":{"displayName":"Owakamare Princewill","userId":"11234081405779952785"},"user_tz":300},"id":"lAPImgA30TSV"},"outputs":[],"source":["#lets check our tokenization for a few samples\n","tokenized_train[0:2]"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":14902,"status":"aborted","timestamp":1754341412886,"user":{"displayName":"Owakamare Princewill","userId":"11234081405779952785"},"user_tz":300},"id":"Ff_viz6m0VWt"},"outputs":[],"source":["# #lets check our tokenization for a few samples\n","# tokenized_test[0:2]"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":14903,"status":"aborted","timestamp":1754341412889,"user":{"displayName":"Owakamare Princewill","userId":"11234081405779952785"},"user_tz":300},"id":"vxFTkIQL0ZE8"},"outputs":[],"source":["tokenized_validation[0:2]"]},{"cell_type":"markdown","metadata":{"id":"vaABf7NQ0ccF"},"source":["# Using DataLoader to Batchify data\n","Make sure to send your datasets to the Dataloader in order to segment your dataset into batches. Remember that we need batches to run our iterative optimization procedure which is typically some form of Mini-batch Gradient Descent.\n","\n","In the interest of time we want to finetune our model on a sample of the training set with 309 records instead of the entire 62K sample size"]},{"cell_type":"markdown","metadata":{"id":"zwQFvswUwr6Q"},"source":["## batch_size"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":14904,"status":"aborted","timestamp":1754341412890,"user":{"displayName":"Owakamare Princewill","userId":"11234081405779952785"},"user_tz":300},"id":"GEKYDi9OvFDg"},"outputs":[],"source":["batch_size = 4"]},{"cell_type":"code","source":["# dataset = load_dataset(\"SetFit/SentEval-CR\")\n","tokenized_train"],"metadata":{"id":"9StknLNPX48P","executionInfo":{"status":"aborted","timestamp":1754341412912,"user_tz":300,"elapsed":14925,"user":{"displayName":"Owakamare Princewill","userId":"11234081405779952785"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# df = pd.read_csv('/content/gdrive/MyDrive/Group 3: palm oil topic classifier/Data/Labeled Data/BERTopic_results.csv')\n","# test = pd.read_csv('/content/gdrive/MyDrive/Group 3: palm oil topic classifier/Data/Labeled Data/BERTopic_TEST.csv')\n","# train = pd.read_csv('/content/gdrive/MyDrive/Group 3: palm oil topic classifier/Text Classification Models/Classification Data/Base/BERTopic_TRAIN_80.csv')\n","# validation = pd.read_csv('/content/gdrive/MyDrive/Group 3: palm oil topic classifier/Text Classification Models/Classification Data/Base/BERTopic_VAL_20.csv')\n","\n","# turning into hugging face format\n","test = Dataset.from_pandas(test)\n","train = Dataset.from_pandas(train)\n","validation = Dataset.from_pandas(validation)"],"metadata":{"id":"525ve9Rwnc5Q","executionInfo":{"status":"aborted","timestamp":1754341412931,"user_tz":300,"elapsed":14944,"user":{"displayName":"Owakamare Princewill","userId":"11234081405779952785"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":14946,"status":"aborted","timestamp":1754341412934,"user":{"displayName":"Owakamare Princewill","userId":"11234081405779952785"},"user_tz":300},"id":"PdN0FUh30dDQ"},"outputs":[],"source":["train_dataset = tokenized_train.shuffle(seed=1111).select(range(305))\n","train_dataloader = DataLoader(train_dataset, batch_size=batch_size)\n","eval_dataloader = DataLoader(tokenized_validation, batch_size=batch_size)"]},{"cell_type":"code","source":["# # Select N examples per class (8 in this case)\n","# train_ds = tokenized_train.shuffle(seed=42).select(range(8 * 6))\n","# test_ds = dataset[\"test\"]"],"metadata":{"id":"SbMF9pw5a3S3","executionInfo":{"status":"aborted","timestamp":1754341412937,"user_tz":300,"elapsed":14949,"user":{"displayName":"Owakamare Princewill","userId":"11234081405779952785"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OKF9c9ro0fXf"},"source":["# Training and Validation\n","We have now gone through all the required preprocessing to prep the data for training. Instead of the Trainer module, it will be a good practice to initially write our own training loops so that we are mindful of all the steps that required for training neural networks.\n","\n","Other than our training and validation data we need to select:\n","\n","An optimizer to run backpropagation\n","A scheduler that sets a protocol for parameter updates at the end of a batch\n","We would also like to set a seed at the start of computation. This ensures that we are able to generate reproducicble results across multiple training sessions.\n","\n","We run validation at the end of each epoch.\n","\n","**At the end of this step we want to report the best validation loss obtained during training. We also want to save the model corresponding to the epoch that reported the best validation loss.**"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":14951,"status":"aborted","timestamp":1754341412940,"user":{"displayName":"Owakamare Princewill","userId":"11234081405779952785"},"user_tz":300},"id":"alRjbWoY2a4i"},"outputs":[],"source":["model_path = '/content/gdrive/MyDrive/Group 3: palm oil topic classifier/Text Classification Models/Spanbert_Saved_Models/Spanbert '"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YOZJHka90hcG","executionInfo":{"status":"aborted","timestamp":1754341412941,"user_tz":300,"elapsed":14951,"user":{"displayName":"Owakamare Princewill","userId":"11234081405779952785"}}},"outputs":[],"source":["from torch.nn.utils import clip_grad_norm_\n","set_seed(42)\n","\n","# Use GPU if available\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","model = AutoModelForSequenceClassification.from_pretrained(\n","    \"SpanBERT/spanbert-large-cased\",\n","    num_labels=6\n",").to(device)\n","\n","num_epochs = 20\n","num_training_steps = len(train_dataloader) * num_epochs\n","\n","# optimizer = AdamW(model.parameters(), lr=5e-5, weight_decay=0.01)\n","optimizer = AdamW(model.parameters(), lr=1e-5, weight_decay=0.005)  # Halved Learning Rate\n","\n","# lr_scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n","lr_scheduler = get_linear_schedule_with_warmup(\n","    optimizer,\n","    num_warmup_steps=int(0.1 * num_training_steps),  # 10% warmup\n","    num_training_steps=num_training_steps\n",")\n","\n","# For Graph\n","x_epochs = []\n","y_train = []\n","y_val = []\n","\n","best_val_loss = float(\"inf\")\n","\n","early_stopping = True\n","early_count = 0 # +1 every time validation loss doesnt improve\n","\n","progress_bar = tqdm(range(num_training_steps))\n","for epoch in range(num_epochs):\n","    x_epochs.append(epoch)\n","\n","    # training\n","    model.train()\n","    training_losses = []\n","    for batch_i, batch in enumerate(train_dataloader):\n","\n","        optimizer.zero_grad()\n","\n","        # copy input to device\n","        input_ids = batch['input_ids'].to(device)\n","        attention_mask = batch['attention_mask'].to(device)\n","        labels = batch['labels'].to(device)\n","\n","        # output = model(**batch)\n","        output = model(input_ids, attention_mask=attention_mask, labels=labels)\n","        training_loss = output.loss\n","        training_losses.append(training_loss.item())\n","\n","        #backprop and update params by taking an optimization step\n","        output.loss.backward()\n","        optimizer.step()\n","        lr_scheduler.step()\n","        progress_bar.update(1)\n","    print(f\"Epoch {epoch}:\")\n","    print(\"Mean Training Loss\", np.mean(training_losses))\n","    y_train.append(np.mean(training_losses))\n","\n","    # validation\n","    val_loss = 0\n","    model.eval() #important to call because we dont want to collect gradients\n","    for batch_i, batch in enumerate(eval_dataloader):\n","        with torch.no_grad():\n","            # copy input to device\n","            input_ids = batch['input_ids'].to(device)\n","            attention_mask = batch['attention_mask'].to(device)\n","            labels = batch['labels'].to(device)\n","            # output = model(**batch)\n","            output = model(input_ids, attention_mask=attention_mask, labels=labels)\n","        val_loss += output.loss\n","\n","    avg_val_loss = val_loss / len(eval_dataloader)\n","    print(f\"Validation loss: {avg_val_loss}\")\n","    y_val.append(avg_val_loss.cpu())\n","\n","    if avg_val_loss < best_val_loss:\n","        print(\"Saving checkpoint!\")\n","        early_count = 0 # Reset counter\n","        best_val_loss = avg_val_loss\n","        torch.save({\n","            'epoch': epoch,\n","            'model_state_dict': model.state_dict(),\n","            # 'optimizer_state_dict': optimizer.state_dict(),\n","            'val_loss': best_val_loss,\n","            },\n","            f\"{model_path}best_model.pt\"\n","        )\n","    elif early_stopping:\n","        early_count += 1\n","\n","        if early_count == 5:\n","            print(f\"Validation loss has not improved for {early_count} iterations; Early Stopping.\")\n","            break\n","\n","    print()"]},{"cell_type":"code","source":["# model = SetFitModel.from_pretrained(\"sentence-transformers/paraphrase-mpnet-base-v2\")\n","\n","# trainer = SetFitTrainer(\n","#     model=model,\n","#     train_dataset=train_ds,\n","#     eval_dataset=test_ds,\n","#     loss_class=CosineSimilarityLoss,\n","#     batch_size=16,\n","#     num_iterations=20,\n","#     num_epochs=1,\n","#     learning_rate=1e-5,   # <-- adjust this\n","#     weight_decay=0.005     # <-- optional\n","# )\n","\n","# # For Graph\n","# x_epochs = []\n","# y_train = []\n","# y_val = []\n","\n","# best_val_loss = float(\"inf\")\n","\n","# early_stopping = True\n","# early_count = 0 # +1 every time validation loss doesnt improve\n","\n","# progress_bar = tqdm(range(num_training_steps))\n","# for epoch in range(num_epochs):\n","#     x_epochs.append(epoch)\n","\n","#     # training\n","#     trainer.train()\n","#     metrics = trainer.evaluate()\n","#     training_losses = []\n","#     for batch_i, batch in enumerate(train_dataloader):\n","\n","#         optimizer.zero_grad()\n","\n","#         # copy input to device\n","#         input_ids = batch['input_ids'].to(device)\n","#         attention_mask = batch['attention_mask'].to(device)\n","#         labels = batch['labels'].to(device)\n","\n","#         # output = model(**batch)\n","#         output = model(input_ids, attention_mask=attention_mask, labels=labels)\n","#         training_loss = output.loss\n","#         training_losses.append(training_loss.item())\n","\n","#         #backprop and update params by taking an optimization step\n","#         output.loss.backward()\n","#         optimizer.step()\n","#         lr_scheduler.step()\n","#         progress_bar.update(1)\n","#     print(f\"Epoch {epoch}:\")\n","#     print(\"Mean Training Loss\", np.mean(training_losses))\n","#     y_train.append(np.mean(training_losses))\n","\n","#     # validation\n","#     val_loss = 0\n","#     model.eval() #important to call because we dont want to collect gradients\n","#     for batch_i, batch in enumerate(eval_dataloader):\n","#         with torch.no_grad():\n","#             # copy input to device\n","#             input_ids = batch['input_ids'].to(device)\n","#             attention_mask = batch['attention_mask'].to(device)\n","#             labels = batch['labels'].to(device)\n","#             # output = model(**batch)\n","#             output = model(input_ids, attention_mask=attention_mask, labels=labels)\n","#         val_loss += output.loss\n","\n","#     avg_val_loss = val_loss / len(eval_dataloader)\n","#     print(f\"Validation loss: {avg_val_loss}\")\n","#     y_val.append(avg_val_loss.cpu())\n","\n","#     if avg_val_loss < best_val_loss:\n","#         print(\"Saving checkpoint!\")\n","#         early_count = 0 # Reset counter\n","#         best_val_loss = avg_val_loss\n","#         torch.save({\n","#             'epoch': epoch,\n","#             'model_state_dict': model.state_dict(),\n","#             # 'optimizer_state_dict': optimizer.state_dict(),\n","#             'val_loss': best_val_loss,\n","#             },\n","#             f\"{model_path}best_model.pt\"\n","#         )\n","#     elif early_stopping:\n","#         early_count += 1\n","\n","#         if early_count == 5:\n","#             print(f\"Validation loss has not improved for {early_count} iterations; Early Stopping.\")\n","#             break\n","\n","#     print()"],"metadata":{"id":"K_O-B_S8db7z","executionInfo":{"status":"aborted","timestamp":1754341412944,"user_tz":300,"elapsed":14954,"user":{"displayName":"Owakamare Princewill","userId":"11234081405779952785"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install optuna"],"metadata":{"id":"BKN97TuvEKvd","executionInfo":{"status":"aborted","timestamp":1754341412946,"user_tz":300,"elapsed":14956,"user":{"displayName":"Owakamare Princewill","userId":"11234081405779952785"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"stpnNywxC-Dx","executionInfo":{"status":"aborted","timestamp":1754341412948,"user_tz":300,"elapsed":14957,"user":{"displayName":"Owakamare Princewill","userId":"11234081405779952785"}}},"outputs":[],"source":["# plt.plot(x_epochs, y_val)\n","\n","plt.plot(x_epochs, y_val, label=\"Validation Loss\")\n","plt.plot(x_epochs, y_train, label=\"Mean Training Loss\")\n","plt.legend()\n","plt.style.use('fivethirtyeight')\n","plt.title(\"Validation Loss\")\n","plt.xlabel(\"Epochs\")"]},{"cell_type":"code","source":["# Load the best model\n","model = AutoModelForSequenceClassification.from_pretrained(\n","    \"SpanBERT/spanbert-large-cased\",\n","    num_labels=6\n",").to(device)\n","\n","best_model_path = f\"{model_path}best_model.pt\"\n","checkpoint = torch.load(best_model_path, map_location=device)\n","model.load_state_dict(checkpoint['model_state_dict'])\n","model.eval() # Set the model to evaluation mode\n","\n","# Create a DataLoader for the test set\n","test_dataloader = DataLoader(tokenized_test, batch_size=batch_size)\n","\n","predictions = []\n","pk_values = [] # To store pk values\n","\n","with torch.no_grad():\n","    for batch in tqdm(test_dataloader, desc=\"Predicting on test data\"):\n","        input_ids = batch['input_ids'].to(device)\n","        attention_mask = batch['attention_mask'].to(device)\n","        # Move pk to CPU and convert to numpy\n","        pk = batch['pk'].cpu().numpy()\n","        pk_values.extend(pk)\n","\n","\n","        outputs = model(input_ids, attention_mask=attention_mask)\n","        logits = outputs.logits\n","\n","        # Apply sigmoid to get probabilities\n","        probs = torch.sigmoid(logits)\n","        predictions.extend(probs.cpu().numpy())\n","\n","# Convert predictions to a DataFrame\n","prediction_df = pd.DataFrame(predictions, columns=[f'Topic_{i}_Prob' for i in range(6)])\n","\n","# Add the original text and pk back to the prediction DataFrame\n","# We need to get the original text and pk from the test_df or tokenized_test dataset\n","# Since tokenized_test has 'pk' and we can get 'summary' from test_df using 'pk'\n","# Let's create a mapping from pk to summary from the original test_df\n","pk_to_text = test_df.set_index('pk')['summary'].to_dict()\n","\n","# Add 'pk' and 'summary' to the prediction_df\n","prediction_df['pk'] = pk_values # Use the collected numpy array of pk values\n","prediction_df['summary'] = prediction_df['pk'].map(pk_to_text)\n","\n","# Rearrange columns to have pk and summary first\n","cols = ['pk', 'summary'] + [col for col in prediction_df.columns if col not in ['pk', 'summary']]\n","prediction_df = prediction_df[cols]\n","\n","display(prediction_df.head())"],"metadata":{"id":"b8UV23x0uEIF","executionInfo":{"status":"aborted","timestamp":1754341412951,"user_tz":300,"elapsed":14960,"user":{"displayName":"Owakamare Princewill","userId":"11234081405779952785"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# # Modify the location per model\n","# model_location = \"/content/gdrive/MyDrive/Group 3: palm oil topic classifier/Text Classification Models/Spanbert_Saved_Models/Spanbert best_model.pt\"\n","# checkpoint = torch.load(model_location)\n","\n","# # Recreate the model architecture\n","# model = AutoModelForSequenceClassification.from_pretrained(\n","#     \"SpanBERT/spanbert-large-cased\",\n","#     num_labels=6\n","# ).to(device)\n","# #\n","# # Load the saved weights\n","# model.load_state_dict(checkpoint[\"model_state_dict\"])\n","# model.to(device)\n","# model.eval()"],"metadata":{"id":"BXoWcRyN-xMQ","executionInfo":{"status":"aborted","timestamp":1754341412952,"user_tz":300,"elapsed":14961,"user":{"displayName":"Owakamare Princewill","userId":"11234081405779952785"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# from sklearn.model_selection import train_test_split\n","\n","# # Assuming dominant_topic_df is the DataFrame you want to split\n","# # Define the features (X) and the target (y)\n","# # Here, we'll use the 'Text' column as the feature\n","# X = dominant_topic_df['Text']\n","\n","# # And the topic probability columns and Dominant_Topic as the target (y)\n","# # Changed to include Topic_0 through Topic_5 instead of the percentage columns\n","# topic_columns = [col for col in dominant_topic_df.columns if col.startswith('Topic_') and len(col) == 7 and not col.endswith('_Perc')]\n","# y = dominant_topic_df[['Document_Num'] + topic_columns]\n","\n","\n","# # Perform the 80/20 split\n","# X_train, X_val, y_train, y_val = train_test_split(\n","#     X, y,\n","#     test_size=0.2,  # 20% for validation\n","#     random_state=random_seed # Use the predefined random_seed for reproducibility\n","#     # Stratify is not directly applicable to multi-output targets like this,\n","#     # so we will remove it.\n","# )\n","\n","# print(\"Training set size:\", len(X_train))\n","# print(\"Validation set size:\", len(X_val))\n","# print(\"\\nShape of X_train:\", X_train.shape)\n","# print(\"Shape of X_val:\", X_val.shape)\n","# print(\"Shape of y_train:\", y_train.shape)\n","# print(\"Shape of y_val:\", y_val.shape)\n","\n","# y"],"metadata":{"id":"gE6oVHl1GiWG","executionInfo":{"status":"aborted","timestamp":1754341412954,"user_tz":300,"elapsed":14962,"user":{"displayName":"Owakamare Princewill","userId":"11234081405779952785"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_df = pd.read_csv('/content/gdrive/MyDrive/Group 3: palm oil topic classifier/Text Classification Models/classified_grievances_multilabel.csv')\n","\n","test_df = test_df[['pk', 'summary', 'Topic_0', 'Topic_1', 'Topic_2', 'Topic_3', 'Topic_4', \"Topic_5\"]]"],"metadata":{"id":"hfFdF7h6HH1M","executionInfo":{"status":"aborted","timestamp":1754341412955,"user_tz":300,"elapsed":14963,"user":{"displayName":"Owakamare Princewill","userId":"11234081405779952785"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nx_kxGNw7ay8"},"source":["#### **Evaluate your model on Test Data**\n","\n","Now we use our finetuned model to evaluate the test set. We use performance metrics from `sklearn.metrics` to test the effectiveness of our model on unseen test data.\n","\n","In order to do that, run the finetuned model you have just saved on your test data and report the following performance metrics:\n","\n","\n","\n","*   Accuracy\n","*   F1 Score"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VsumZmDG7aTi","executionInfo":{"status":"aborted","timestamp":1754341412973,"user_tz":300,"elapsed":14980,"user":{"displayName":"Owakamare Princewill","userId":"11234081405779952785"}}},"outputs":[],"source":["eval_dataloader = DataLoader(tokenized_validation, batch_size=len(tokenized_validation))\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","model.eval()\n","test_batch_logits = []\n","y_true = []\n","for batch_i, batch in enumerate(eval_dataloader):\n","    with torch.no_grad():\n","        # copy input to device\n","        input_ids = batch['input_ids'].to(device)\n","        attention_mask = batch['attention_mask'].to(device)\n","        labels = batch['labels'].cpu().detach().numpy()\n","\n","        # output = model(**batch)\n","        output = model(input_ids, attention_mask=attention_mask)\n","        test_batch_logits.append(output.logits)\n","        y_true.extend(labels)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oGEY2QKq7u8z","executionInfo":{"status":"aborted","timestamp":1754341412975,"user_tz":300,"elapsed":14982,"user":{"displayName":"Owakamare Princewill","userId":"11234081405779952785"}}},"outputs":[],"source":["print(len(test_batch_logits),len(eval_dataloader))\n","test_logits = torch.cat(test_batch_logits, dim=0)\n","\n","#sanity check -> dimension 0 of your logits tensor should be same as the size of the test dataset\n","print(test_logits.shape,len(tokenized_validation),len(y_true))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KvFRL0nx7yqk","executionInfo":{"status":"aborted","timestamp":1754341412976,"user_tz":300,"elapsed":14983,"user":{"displayName":"Owakamare Princewill","userId":"11234081405779952785"}}},"outputs":[],"source":["#Convert the logits to predicted labels\n","y_pred = torch.argmax(test_logits, dim = 1).cpu().numpy()\n","print(y_true[:10])\n","print(y_pred[:10])\n","\n","#sanity check: should have as many predictions as labels\n","assert len(y_pred)==len(y_true)"]},{"cell_type":"markdown","metadata":{"id":"R-iQk4V5uceI"},"source":["## F1 & Accuracy Scores"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UGiYv47k71F7","executionInfo":{"status":"aborted","timestamp":1754341412977,"user_tz":300,"elapsed":14983,"user":{"displayName":"Owakamare Princewill","userId":"11234081405779952785"}}},"outputs":[],"source":["print('F1 Score (macro):', f1_score(y_true, y_pred, average='macro'))\n","print('F1 Score (weighted):', f1_score(y_true, y_pred, average='weighted'))\n","print('F1 Score (micro):', f1_score(y_true, y_pred, average='micro'))\n","print('Accuracy Score:', accuracy_score(y_true, y_pred))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UkOq07CqUHrS","executionInfo":{"status":"aborted","timestamp":1754341412980,"user_tz":300,"elapsed":14986,"user":{"displayName":"Owakamare Princewill","userId":"11234081405779952785"}}},"outputs":[],"source":["from sklearn.metrics import classification_report\n","\n","print(classification_report(y_true, y_pred, target_names=['Failed Compensation/Land Rights??? (0)',\n","                                                          'Environmental Impact??? (1)',\n","                                                          'Administrative??? (2)',\n","                                                          'Deforestation??? (3)',\n","                                                          'Labour Rights (4)',\n","                                                          'Illegal or Contaminated FFB (5)']))"]},{"cell_type":"code","source":["from sklearn.metrics import classification_report\n","\n","print(classification_report(y_true, y_pred, target_names=['0','1','2','3','4','5']))"],"metadata":{"id":"or8TyS8OPn87","executionInfo":{"status":"aborted","timestamp":1754341412994,"user_tz":300,"elapsed":15000,"user":{"displayName":"Owakamare Princewill","userId":"11234081405779952785"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"c7bSfSOt0Czz"},"source":["# Predicting"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PY3jGsFC0NMg","executionInfo":{"status":"aborted","timestamp":1754341412996,"user_tz":300,"elapsed":15001,"user":{"displayName":"Owakamare Princewill","userId":"11234081405779952785"}}},"outputs":[],"source":["model_name = \"SpanBERT\""]},{"cell_type":"markdown","source":["## Evaluate"],"metadata":{"id":"LhpsYQINEUUl"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"0odXwOv20gUm","executionInfo":{"status":"aborted","timestamp":1754341412997,"user_tz":300,"elapsed":15002,"user":{"displayName":"Owakamare Princewill","userId":"11234081405779952785"}}},"outputs":[],"source":["# Load new data for classification\n","new_data_path = '/content/gdrive/MyDrive/Group 3: palm oil topic classifier/Data/NEW_grievances_formatted.csv'\n","new_df = pd.read_csv(new_data_path)\n","\n","print(f\"Loaded {len(new_df)} entries for classification\")\n","print(\"Columns in new data:\", new_df.columns.tolist())\n","\n","# Prepare the text data (assuming 'summary' column contains the text to classify)\n","# If your text column has a different name, change 'summary' below\n","text_column = 'Text'\n","\n","# Convert to Dataset format\n","new_dataset = Dataset.from_pandas(new_df[[text_column]])\n","\n","# Tokenize the new data\n","tokenized_new_data = new_dataset.map(\n","    lambda example: tokenizer(str(example[text_column]),  # Convert to string to handle NaN\n","                             padding=\"max_length\",\n","                             truncation=True,\n","                             max_length=tokenizer_length)\n",")\n","\n","# Set format to torch tensors\n","tokenized_new_data.set_format(\"torch\")\n","\n","# Create DataLoader for inference\n","inference_dataloader = DataLoader(tokenized_new_data, batch_size=batch_size)\n","\n","# Run inference\n","model.eval()   # LOAD BEST MSODEL\n","all_predictions = []\n","\n","with torch.no_grad():\n","    for batch in tqdm(inference_dataloader, desc=\"Classifying\"):\n","        # Move batch to device\n","        input_ids = batch['input_ids'].to(device)\n","        attention_mask = batch['attention_mask'].to(device)\n","\n","        # Get model predictions\n","        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n","        logits = outputs.logits\n","\n","        # Convert logits to predicted labels\n","        predictions = torch.argmax(logits, dim=1).cpu().numpy()\n","        all_predictions.extend(predictions)\n","\n","\n","# Read Classification CSV\n","results_df = pd.read_csv('/content/gdrive/MyDrive/Group 3: palm oil topic classifier/Text Classification Models/classified_grievances.csv')\n","results_df[f'{model_name}_label'] = all_predictions[:120]\n","\n","\n","print(f\"\\nClassification complete!\")\n","print(f\"Label distribution:\")\n","# Add Label\n","print(results_df[f'{model_name}_label'].value_counts().sort_index())\n","\n","output_path = '/content/gdrive/MyDrive/Group 3: palm oil topic classifier/Text Classification Models/classified_grievances.csv'\n","results_df.to_csv(output_path, index=False)\n","\n","print(\"\\nFirst 10 classified entries:\")\n","print(results_df[['summary', f'{model_name}_label']].head(10))"]},{"cell_type":"code","source":["results_df"],"metadata":{"id":"HB1B-zlnPtnS","executionInfo":{"status":"aborted","timestamp":1754341412999,"user_tz":300,"elapsed":15004,"user":{"displayName":"Owakamare Princewill","userId":"11234081405779952785"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gDw79hu80jka","executionInfo":{"status":"aborted","timestamp":1754341413001,"user_tz":300,"elapsed":15005,"user":{"displayName":"Owakamare Princewill","userId":"11234081405779952785"}}},"outputs":[],"source":["label_map = {\n","    0: 'Failed Compensation/Land Rights',\n","    1: 'Environmental Impact',\n","    2: 'Administrative',\n","    3: 'Deforestation',\n","    4: 'Labour Rights',\n","    5: 'Illegal or Contaminated FFB'\n","}\n","\n","results_df[f'{model_name}_topic'] = results_df[f'{model_name}_label'].map(label_map)\n","results_df"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ixI3QzAM0mXl","executionInfo":{"status":"aborted","timestamp":1754341413002,"user_tz":300,"elapsed":15006,"user":{"displayName":"Owakamare Princewill","userId":"11234081405779952785"}}},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","topic_counts = results_df[f'{model_name}_topic'].value_counts()\n","print(topic_counts)\n","\n","plt.figure(figsize=(10, 6))\n","topic_counts.plot(kind='bar', edgecolor='black')\n","plt.title('Number of Grievances per Topic')\n","plt.xlabel('Topic')\n","plt.ylabel('Count')\n","plt.xticks(rotation=45, ha='right')\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SnxD1zq40ohW","executionInfo":{"status":"aborted","timestamp":1754341413003,"user_tz":300,"elapsed":15007,"user":{"displayName":"Owakamare Princewill","userId":"11234081405779952785"}}},"outputs":[],"source":["results_df[results_df[f'{model_name}_label'] == 0]"]},{"cell_type":"markdown","source":["## Comparison between Manual Labels and Transforemers' labels"],"metadata":{"id":"iVL0VrQUP5SR"}},{"cell_type":"code","source":["results_df"],"metadata":{"id":"VJ_du5RtnD6B","executionInfo":{"status":"aborted","timestamp":1754341413004,"user_tz":300,"elapsed":15008,"user":{"displayName":"Owakamare Princewill","userId":"11234081405779952785"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","import pandas as pd\n","\n","# Get counts\n","manual_counts = results_df['manual_label'].value_counts()\n","distilbert_counts = results_df['DistilBERT_label'].value_counts()\n","spanbert_counts = results_df['SpanBERT_label'].value_counts()\n","electra_counts = results_df['Electra_label'].value_counts()\n","\n","\n","combined_counts = pd.DataFrame({\n","    'Manual': manual_counts,\n","    'DistilBERT': distilbert_counts,\n","    'SpanBERT': spanbert_counts\n","}).fillna(0)\n","\n","combined_counts = combined_counts.sort_index()\n","\n","# Plot\n","combined_counts.plot(kind='bar', figsize=(12, 6), edgecolor='black')\n","plt.title('Number of Grievances per Topic: Manual vs DistilBERT vs SpanBERT')\n","plt.xlabel('Topic')\n","plt.ylabel('Count')\n","plt.xticks(rotation=45, ha='right')\n","plt.legend(title='Label')\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"Fp6Q-HZ9P3HN","executionInfo":{"status":"aborted","timestamp":1754341413018,"user_tz":300,"elapsed":15021,"user":{"displayName":"Owakamare Princewill","userId":"11234081405779952785"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Accuracy and F1 for Testing Data"],"metadata":{"id":"P4RQjzQFGdsK"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"SetWYI36t7DM","executionInfo":{"status":"aborted","timestamp":1754341413020,"user_tz":300,"elapsed":15023,"user":{"displayName":"Owakamare Princewill","userId":"11234081405779952785"}}},"outputs":[],"source":["# --- Evaluation ---\n","true_labels = results_df['manual_label'].astype(int).values\n","pred_labels = np.array(all_predictions)\n","\n","print(\"Evaluation on New Data:\")\n","print(\"F1 Score (macro):\", f1_score(true_labels, pred_labels, average='macro'))\n","print(\"F1 Score (weighted):\", f1_score(true_labels, pred_labels, average='weighted'))\n","print(\"F1 Score (micro):\", f1_score(true_labels, pred_labels, average='micro'))\n","print(\"Accuracy:\", accuracy_score(true_labels, pred_labels))\n","\n","# Classification Report\n","\n","print(\"\\nClassification Report:\")\n","print(classification_report(true_labels, pred_labels, target_names=[\n","    'Failed Compensation/Land Rights',\n","    'Environmental Impact',\n","    'Administrative',\n","    'Deforestation',\n","    'Labour Rights',\n","    'Illegal or Contaminated FFB'\n","]))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EkEl9urtw6g3","executionInfo":{"status":"aborted","timestamp":1754341413021,"user_tz":300,"elapsed":15023,"user":{"displayName":"Owakamare Princewill","userId":"11234081405779952785"}}},"outputs":[],"source":["# Modify the location per model\n","model_location = \"/content/gdrive/MyDrive/Group 3: palm oil topic classifier/Text Classification Models/Spanbert_Saved_Models/Spanbert best_model.pt\"\n","checkpoint = torch.load(model_location)\n","\n","# Recreate the model architecture\n","model = AutoModelForSequenceClassification.from_pretrained(\n","    \"SpanBERT/spanbert-large-cased\",\n","    num_labels=6\n",").to(device)\n","\n","# Load the saved weights\n","model.load_state_dict(checkpoint[\"model_state_dict\"])\n","model.to(device)\n","model.eval()"]},{"cell_type":"code","source":["for i in range(6):\n","  print(f\"Length of results with manual label {i} is: {len(results_df[results_df['manual_label'] == i])}\")"],"metadata":{"id":"OQAtROPSQQ94","executionInfo":{"status":"aborted","timestamp":1754341413022,"user_tz":300,"elapsed":15023,"user":{"displayName":"Owakamare Princewill","userId":"11234081405779952785"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load the best model\n","best_model_path = f\"{model_path}best_model.pt\"\n","checkpoint = torch.load(best_model_path, map_location=device)\n","model.load_state_dict(checkpoint['model_state_dict'])\n","model.eval() # Set the model to evaluation mode\n","\n","# Create a DataLoader for the test set\n","test_dataloader = DataLoader(tokenized_test, batch_size=batch_size)\n","\n","predictions = []\n","pk_values = [] # To store pk values\n","\n","with torch.no_grad():\n","    for batch in tqdm(test_dataloader, desc=\"Predicting on test data\"):\n","        input_ids = batch['input_ids'].to(device)\n","        attention_mask = batch['attention_mask'].to(device)\n","        # Move pk to CPU and convert to numpy\n","        pk = batch['pk'].cpu().numpy()\n","        pk_values.extend(pk)\n","\n","\n","        outputs = model(input_ids, attention_mask=attention_mask)\n","        logits = outputs.logits\n","\n","        # Apply sigmoid to get probabilities\n","        probs = torch.sigmoid(logits)\n","        predictions.extend(probs.cpu().numpy())\n","\n","# Convert predictions to a DataFrame\n","prediction_df = pd.DataFrame(predictions, columns=[f'Topic_{i}_Prob' for i in range(6)])\n","\n","# Add the original text and pk back to the prediction DataFrame\n","# We need to get the original text and pk from the test_df or tokenized_test dataset\n","# Since tokenized_test has 'pk' and we can get 'summary' from test_df using 'pk'\n","# Let's create a mapping from pk to summary from the original test_df\n","pk_to_text = test_df.set_index('pk')['summary'].to_dict()\n","\n","# Add 'pk' and 'summary' to the prediction_df\n","prediction_df['pk'] = pk_values # Use the collected numpy array of pk values\n","prediction_df['summary'] = prediction_df['pk'].map(pk_to_text)\n","\n","# Rearrange columns to have pk and summary first\n","cols = ['pk', 'summary'] + [col for col in prediction_df.columns if col not in ['pk', 'summary']]\n","prediction_df = prediction_df[cols]\n","\n","display(prediction_df.head())"],"metadata":{"id":"7Sm7ZtWQHZc5","executionInfo":{"status":"aborted","timestamp":1754341413024,"user_tz":300,"elapsed":15025,"user":{"displayName":"Owakamare Princewill","userId":"11234081405779952785"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import accuracy_score, f1_score\n","\n","# Define the threshold for converting probabilities to binary labels\n","evaluation_threshold = 0.5\n","\n","# Get the predicted probabilities for the topic columns from prediction_df\n","topic_prob_columns = [col for col in prediction_df.columns if col.startswith('Topic_') and col.endswith('_Prob')]\n","predicted_probs = prediction_df[topic_prob_columns].values\n","\n","# Convert probabilities to binary predictions using the threshold\n","binary_predictions = (predicted_probs >= evaluation_threshold).astype(int)\n","\n","# Get the true multi-hot encoded labels from test_df\n","# Ensure alignment with prediction_df based on 'pk'\n","# Assuming test_df contains 'pk' and 'Topic_0' through 'Topic_5'\n","true_labels_for_eval = pd.merge(prediction_df[['pk']], test_df[['pk'] + [f'Topic_{i}' for i in range(6)]], on='pk', how='left')\n","\n","# Extract the true labels in the correct order\n","true_labels_values = true_labels_for_eval[[f'Topic_{i}' for i in range(6)]].values\n","\n","# Create a DataFrame for binary predictions (optional, but requested)\n","binary_prediction_df = pd.DataFrame(binary_predictions, columns=[f'Topic_{i}' for i in range(6)])\n","# Add pk for potential merging or inspection\n","binary_prediction_df['pk'] = prediction_df['pk']\n","# Rearrange columns\n","cols = ['pk'] + [col for col in binary_prediction_df.columns if col != 'pk']\n","binary_prediction_df = binary_prediction_df[cols]\n","\n","# Display the head of the binary predictions DataFrame\n","print(\"Binary Predictions (first 5 rows):\")\n","display(binary_prediction_df.head(20))\n","\n","# Calculate Accuracy\n","accuracy = accuracy_score(true_labels_values, binary_predictions)\n","\n","# Calculate F1 Score (using micro average for multi-label)\n","f1 = f1_score(true_labels_values, binary_predictions, average='micro')\n","\n","print(f\"\\nEvaluation Results (Threshold = {evaluation_threshold}):\")\n","print(f\"Accuracy: {accuracy:.4f}\")\n","print(f\"Micro F1 Score: {f1:.4f}\")\n","\n","# Note: You can easily change the 'evaluation_threshold' variable above and re-run this cell\n","# to see the impact on accuracy and F1 score."],"metadata":{"id":"TLXg1tjkHSDY","executionInfo":{"status":"aborted","timestamp":1754341413030,"user_tz":300,"elapsed":15030,"user":{"displayName":"Owakamare Princewill","userId":"11234081405779952785"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Calculate the number of correctly classified topics for each topic\n","correctly_classified_counts = {}\n","total_instances_per_topic = np.sum(true_labels_values, axis=0) # Total true instances for each topic\n","\n","print(\"\\nEvaluation per Topic (Threshold =\", evaluation_threshold, \"):\")\n","print(\"------------------------------------------------------------\")\n","\n","for i in range(6):\n","    topic_col_name = f'Topic_{i}'\n","    # Count where binary prediction matches true label for the current topic\n","    correct_predictions_for_topic = np.sum(binary_predictions[:, i] == true_labels_values[:, i])\n","    correctly_classified_counts[topic_col_name] = correct_predictions_for_topic\n","\n","    # Calculate percentage of correctly classified instances for this topic\n","    # This is the overall accuracy for this specific binary classification task\n","    total_instances = len(true_labels_values) # Total number of documents\n","    percentage_correct = (correct_predictions_for_topic / total_instances) * 100\n","\n","    # Calculate F1 Score for the current label\n","    f1 = f1_score(true_labels_values[:, i], binary_predictions[:, i])\n","\n","\n","    print(f\"{topic_col_name}: Correctly Classified = {correct_predictions_for_topic}, Total Instances = {total_instances}, Percentage Correct = {percentage_correct:.2f}%, F1 Score = {f1:.4f}\")\n","\n","print(\"------------------------------------------------------------\")\n","\n","# Create a bar chart to visualize the percentage of correctly classified instances per topic\n","topics = list(correctly_classified_counts.keys())\n","# Recalculate percentages for plotting as it was not explicitly stored\n","percentages = [(correctly_classified_counts[topic] / len(true_labels_values)) * 100 for topic in topics]\n","\n","\n","plt.figure(figsize=(10, 6))\n","plt.bar(topics, percentages, color='skyblue')\n","plt.title('Percentage of Correctly Classified Topics on Test Data')\n","plt.xlabel('Topic')\n","plt.ylabel('Percentage Correctly Classified')\n","plt.ylim(0, 100) # Set y-axis limit to 100%\n","plt.xticks(rotation=45, ha='right')\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"jWuUONURuzdf","executionInfo":{"status":"aborted","timestamp":1754341413044,"user_tz":300,"elapsed":15043,"user":{"displayName":"Owakamare Princewill","userId":"11234081405779952785"}}},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"L4","machine_shape":"hm","provenance":[{"file_id":"1TgZJawetzacUB0Y9Mkhq3yklRmnyWkub","timestamp":1753716600713}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}