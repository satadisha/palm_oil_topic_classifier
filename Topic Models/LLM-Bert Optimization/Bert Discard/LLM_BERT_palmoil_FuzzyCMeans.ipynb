{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"2rGyA1eI9XIn"},"outputs":[],"source":["import random"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SzdiX34f9Y4H"},"outputs":[],"source":["random_seed = 10\n","random.seed(random_seed)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":96713,"status":"ok","timestamp":1753211563115,"user":{"displayName":"Helena Card","userId":"17940059188897151580"},"user_tz":300},"id":"a9a02be5","outputId":"16d23b19-432f-401d-8c94-3c7e49863dd9"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting swifter\n","  Downloading swifter-1.4.0.tar.gz (1.2 MB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m44.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: pandas>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from swifter) (2.2.2)\n","Requirement already satisfied: psutil>=5.6.6 in /usr/local/lib/python3.11/dist-packages (from swifter) (5.9.5)\n","Requirement already satisfied: dask>=2.10.0 in /usr/local/lib/python3.11/dist-packages (from dask[dataframe]>=2.10.0->swifter) (2025.5.0)\n","Requirement already satisfied: tqdm>=4.33.0 in /usr/local/lib/python3.11/dist-packages (from swifter) (4.67.1)\n","Requirement already satisfied: click>=8.1 in /usr/local/lib/python3.11/dist-packages (from dask>=2.10.0->dask[dataframe]>=2.10.0->swifter) (8.2.1)\n","Requirement already satisfied: cloudpickle>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from dask>=2.10.0->dask[dataframe]>=2.10.0->swifter) (3.1.1)\n","Requirement already satisfied: fsspec>=2021.09.0 in /usr/local/lib/python3.11/dist-packages (from dask>=2.10.0->dask[dataframe]>=2.10.0->swifter) (2025.7.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from dask>=2.10.0->dask[dataframe]>=2.10.0->swifter) (25.0)\n","Requirement already satisfied: partd>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from dask>=2.10.0->dask[dataframe]>=2.10.0->swifter) (1.4.2)\n","Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.11/dist-packages (from dask>=2.10.0->dask[dataframe]>=2.10.0->swifter) (6.0.2)\n","Requirement already satisfied: toolz>=0.10.0 in /usr/local/lib/python3.11/dist-packages (from dask>=2.10.0->dask[dataframe]>=2.10.0->swifter) (0.12.1)\n","Requirement already satisfied: importlib_metadata>=4.13.0 in /usr/local/lib/python3.11/dist-packages (from dask>=2.10.0->dask[dataframe]>=2.10.0->swifter) (8.7.0)\n","Requirement already satisfied: pyarrow>=14.0.1 in /usr/local/lib/python3.11/dist-packages (from dask[dataframe]>=2.10.0->swifter) (18.1.0)\n","Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.0->swifter) (2.0.2)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.0->swifter) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.0->swifter) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.0->swifter) (2025.2)\n","Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib_metadata>=4.13.0->dask>=2.10.0->dask[dataframe]>=2.10.0->swifter) (3.23.0)\n","Requirement already satisfied: locket in /usr/local/lib/python3.11/dist-packages (from partd>=1.4.0->dask>=2.10.0->dask[dataframe]>=2.10.0->swifter) (1.0.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0.0->swifter) (1.17.0)\n","Building wheels for collected packages: swifter\n","  Building wheel for swifter (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for swifter: filename=swifter-1.4.0-py3-none-any.whl size=16505 sha256=5d2f8abe3b5225f88b5bf43ef6fddfc0dbc5c8fdf5df83f4591f8ead380eee2c\n","  Stored in directory: /root/.cache/pip/wheels/ef/7f/bd/9bed48f078f3ee1fa75e0b29b6e0335ce1cb03a38d3443b3a3\n","Successfully built swifter\n","Installing collected packages: swifter\n","Successfully installed swifter-1.4.0\n","Collecting python-dotenv\n","  Downloading python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\n","Downloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n","Installing collected packages: python-dotenv\n","Successfully installed python-dotenv-1.1.1\n","Collecting bertopic\n","  Downloading bertopic-0.17.3-py3-none-any.whl.metadata (24 kB)\n","Requirement already satisfied: hdbscan>=0.8.29 in /usr/local/lib/python3.11/dist-packages (from bertopic) (0.8.40)\n","Requirement already satisfied: umap-learn>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from bertopic) (0.5.9.post2)\n","Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.11/dist-packages (from bertopic) (2.0.2)\n","Requirement already satisfied: pandas>=1.1.5 in /usr/local/lib/python3.11/dist-packages (from bertopic) (2.2.2)\n","Requirement already satisfied: plotly>=4.7.0 in /usr/local/lib/python3.11/dist-packages (from bertopic) (5.24.1)\n","Requirement already satisfied: scikit-learn>=1.0 in /usr/local/lib/python3.11/dist-packages (from bertopic) (1.6.1)\n","Requirement already satisfied: sentence-transformers>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from bertopic) (4.1.0)\n","Requirement already satisfied: tqdm>=4.41.1 in /usr/local/lib/python3.11/dist-packages (from bertopic) (4.67.1)\n","Requirement already satisfied: llvmlite>0.36.0 in /usr/local/lib/python3.11/dist-packages (from bertopic) (0.43.0)\n","Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.11/dist-packages (from hdbscan>=0.8.29->bertopic) (1.16.0)\n","Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.11/dist-packages (from hdbscan>=0.8.29->bertopic) (1.5.1)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.5->bertopic) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.5->bertopic) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.5->bertopic) (2025.2)\n","Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly>=4.7.0->bertopic) (8.5.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from plotly>=4.7.0->bertopic) (25.0)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.0->bertopic) (3.6.0)\n","Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=0.4.1->bertopic) (4.53.2)\n","Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=0.4.1->bertopic) (2.6.0+cu124)\n","Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=0.4.1->bertopic) (0.33.4)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=0.4.1->bertopic) (11.3.0)\n","Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=0.4.1->bertopic) (4.14.1)\n","Requirement already satisfied: numba>=0.51.2 in /usr/local/lib/python3.11/dist-packages (from umap-learn>=0.5.0->bertopic) (0.60.0)\n","Requirement already satisfied: pynndescent>=0.5 in /usr/local/lib/python3.11/dist-packages (from umap-learn>=0.5.0->bertopic) (0.5.13)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (3.18.0)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (2025.7.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (6.0.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (2.32.3)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (1.1.5)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.1.5->bertopic) (1.17.0)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (3.1.6)\n","Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic)\n","  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic)\n","  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic)\n","  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic)\n","  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic)\n","  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic)\n","  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic)\n","  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic)\n","  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic)\n","  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (12.4.127)\n","Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic)\n","  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (3.2.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (1.3.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.4.1->bertopic) (2024.11.6)\n","Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.4.1->bertopic) (0.21.2)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.4.1->bertopic) (0.5.3)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (3.0.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (3.4.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (2025.7.14)\n","Downloading bertopic-0.17.3-py3-none-any.whl (153 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.0/153.0 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m128.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m87.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m39.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m44.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m100.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, bertopic\n","  Attempting uninstall: nvidia-nvjitlink-cu12\n","    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n","    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n","      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n","  Attempting uninstall: nvidia-curand-cu12\n","    Found existing installation: nvidia-curand-cu12 10.3.6.82\n","    Uninstalling nvidia-curand-cu12-10.3.6.82:\n","      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n","  Attempting uninstall: nvidia-cufft-cu12\n","    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n","    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n","      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n","  Attempting uninstall: nvidia-cuda-runtime-cu12\n","    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n","    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n","    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n","    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-cupti-cu12\n","    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n","    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n","  Attempting uninstall: nvidia-cublas-cu12\n","    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n","    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n","      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n","  Attempting uninstall: nvidia-cusparse-cu12\n","    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n","    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n","      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n","  Attempting uninstall: nvidia-cudnn-cu12\n","    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n","    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n","      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n","  Attempting uninstall: nvidia-cusolver-cu12\n","    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n","    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n","      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n","Successfully installed bertopic-0.17.3 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n","Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n","Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n","Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n","Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n","Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.0)\n","Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.5)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.1.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.14.1)\n","Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.73.1)\n","Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n","Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n","Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.0.2)\n","Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.14.0)\n","Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n","Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n","Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n","Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.16.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.7.14)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.8.2)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.2)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n","Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (4.1.0)\n","Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.53.2)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\n","Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0+cu124)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.16.0)\n","Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.33.4)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.3.0)\n","Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.14.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.18.0)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.7.0)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (25.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.5)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (9.1.0.70)\n","Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.5.8)\n","Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.2.1.3)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.5.147)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.6.1.9)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.3.1.170)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n","Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.2)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.5.1)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.7.14)\n","Collecting gensim\n","  Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n","Collecting numpy<2.0,>=1.18.5 (from gensim)\n","  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting scipy<1.14.0,>=1.7.0 (from gensim)\n","  Downloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.3.0.post1)\n","Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n","Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.7/26.7 MB\u001b[0m \u001b[31m91.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m126.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: numpy, scipy, gensim\n","  Attempting uninstall: numpy\n","    Found existing installation: numpy 2.0.2\n","    Uninstalling numpy-2.0.2:\n","      Successfully uninstalled numpy-2.0.2\n","  Attempting uninstall: scipy\n","    Found existing installation: scipy 1.16.0\n","    Uninstalling scipy-1.16.0:\n","      Successfully uninstalled scipy-1.16.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","tsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.13.1 which is incompatible.\n","opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n","thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n","opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n","opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed gensim-4.3.3 numpy-1.26.4 scipy-1.13.1\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["numpy"]},"id":"2ec3bf864bf346e9b371ad643f5b8dc9"}},"metadata":{}}],"source":["%pip install swifter\n","%pip install python-dotenv\n","%pip install bertopic\n","%pip install tensorflow\n","%pip install sentence-transformers\n","%pip install gensim"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oEU1G267dRJ-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"error","timestamp":1753211684578,"user_tz":300,"elapsed":121459,"user":{"displayName":"Helena Card","userId":"17940059188897151580"}},"outputId":"04a26a97-793a-4036-a047-3d97d89f6f1b"},"outputs":[{"output_type":"error","ename":"ValueError","evalue":"mount failed","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-4-1050275017.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/gdrive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    275\u001b[0m             \u001b[0;34m'https://research.google.com/colaboratory/faq.html#drive-timeout'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m         )\n\u001b[0;32m--> 277\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mount failed'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mextra_reason\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m       \u001b[0;31m# Terminate the DriveFS binary before killing bash.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: mount failed"]}],"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qj_liV4ZadVh"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import nltk\n","nltk.download('stopwords')\n","from nltk.corpus import stopwords\n","import string\n","import plotly.express as px\n","import matplotlib.pyplot as plt\n","\n","from datetime import datetime\n","import logging\n","import os\n","import sys\n","import time\n","import re\n","\n","from dotenv import load_dotenv\n","#import openai\n","import swifter\n","from sentence_transformers import SentenceTransformer\n","\n","\n","from bertopic import BERTopic\n","import joblib\n","from sklearn.preprocessing import normalize\n","from wordcloud import WordCloud"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"EImuREcMeOYX"},"outputs":[],"source":["#shared drive version at everyone can set up\n","df = pd.read_csv('/content/gdrive/MyDrive/Group 3: palm oil topic classifier/Data/palm_oil_grievance_logs.csv')\n","df.head(5)"]},{"cell_type":"markdown","metadata":{"id":"AC7D_Wdaf2gF"},"source":["## Preprocessing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_hHQQMR30UN4"},"outputs":[],"source":["#stop words, lower case, and remove punctuation\n","stop_words = set(stopwords.words('english'))\n","punctuation = set(string.punctuation)\n","\n","# preprocessing function\n","def clean_text(text):\n","    # convert non-string types to string for potential na vals\n","    if not isinstance(text, str):\n","        text = str(text)\n","\n","    text = text.lower()\n","    # Use regular expression to replace all punctuation with a space\n","    text = re.sub(f'[{re.escape(\"\".join(punctuation))}]', ' ', text)\n","    tokens = text.split()\n","    # Removed the strip() call as regex handles punctuation removal\n","    # tokens = [word.strip(\"\".join(punctuation)) for word in tokens]\n","    tokens = [word for word in tokens if word and word not in stop_words]\n","    return tokens\n","\n","# Add cleaned tokens to the df\n","df[\"tokens\"] = df[\"summary\"].apply(clean_text)\n","\n","# Define the list of words to filter out\n","words_to_filter = {'nan', 'pt','report','rspo','alleged', 'palm', 'oil', 'company', 'community', 'complainant',\n","                   'companies', 'also', 'without', 'group', 'allegedly', 'period', 'respondent', 'reported',\n","                   'mentioned', '2019', '19', '2020', 'subsidary', 'alleges'}\n","\n","# filter out words that we think are not helpful\n","df['flt_tokens'] = df['tokens'].apply(lambda tokens: [word for word in tokens if word not in words_to_filter])\n","\n","\n","df['flt_tokens'] = df['flt_tokens'].apply(lambda x: ' '.join(x))"]},{"cell_type":"markdown","metadata":{"id":"7Q9XIOjofz6L"},"source":["## Embedding"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RckJN2u6lgc1"},"outputs":[],"source":["logging.basicConfig(format='%(asctime)s: %(levelname)s: %(message)s', level=logging.INFO)\n","\n","BATCH_SIZE = 100\n","output_dir = \"../data/intermediate\"\n","os.makedirs(output_dir, exist_ok=True)\n","\n","# load model once\n","model = SentenceTransformer('all-MiniLM-L6-v2')\n","\n","# define embedding function\n","def get_embedding(text) -> list:\n","    if not text:\n","        return None\n","    try:\n","        return model.encode(text)\n","    except Exception as e:\n","        logging.error(f\"Error getting embedding for text: {str(text)[:50]}... Error: {e}\")\n","        return None\n","\n","# main batch embedding function\n","def run_local_embeddings():\n","    formatted_datetime = datetime.now().strftime(\"%d_%b_%Y_%H_%M_%S\")\n","    n = len(df)\n","\n","    if 'embedding' not in df.columns:\n","        df['embedding'] = None\n","\n","    df_start = 0\n","    while df_start < n:\n","        df_intermediate = df[df_start:df_start + BATCH_SIZE].copy()\n","\n","        # Only process rows that do NOT have numpy array embeddings yet\n","        df_intermediate_unprocessed = df_intermediate.loc[\n","            df_intermediate['embedding'].apply(lambda x: not isinstance(x, np.ndarray))\n","        ]\n","        unprocessed_rows = len(df_intermediate_unprocessed)\n","\n","        if unprocessed_rows == 0:\n","            logging.info(f\"No unprocessed rows in batch starting at {df_start}\")\n","        else:\n","            logging.info(f\"Running embeddings on {unprocessed_rows} rows in batch starting at {df_start}\")\n","            try:\n","                df_intermediate_unprocessed[\"embedding\"] = df_intermediate_unprocessed[\"flt_tokens\"].swifter.apply(\n","                    get_embedding\n","                )\n","\n","                successful = df_intermediate_unprocessed.loc[\n","                    df_intermediate_unprocessed['embedding'].apply(lambda x: isinstance(x, np.ndarray))\n","                ]\n","                df.loc[successful.index, 'embedding'] = successful['embedding']\n","\n","            except Exception as exc:\n","                logging.exception(f\"Exception during embedding: {exc}\")\n","\n","        # save partial batch\n","        df.to_pickle(f\"{output_dir}/embeddings_partial_{df_start}_{formatted_datetime}.pkl\")\n","        df_start += BATCH_SIZE\n","\n","    # save full dataset\n","    df.to_pickle(f\"{output_dir}/embeddings_full_{formatted_datetime}.pkl\")\n","    logging.info(\"Finished all batches and saved full dataset.\")\n","\n","run_local_embeddings()"]},{"cell_type":"markdown","metadata":{"id":"QnYwSu841QD8"},"source":["## Model Creation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w4DdDBYUiA-m"},"outputs":[],"source":["from umap import UMAP\n","from hdbscan import HDBSCAN\n","from bertopic import BERTopic\n","from sklearn.cluster import KMeans\n","from sklearn.mixture import GaussianMixture"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8daed2e1"},"outputs":[],"source":["%pip install scikit-fuzzy"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P2uAluaWBKQi"},"outputs":[],"source":["from skfuzzy.cluster import cmeans\n","from sentence_transformers import SentenceTransformer\n","import umap\n","import numpy as np"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v3H-DzfT05Lt"},"outputs":[],"source":["# # Define Clustering Models\n","# soft_kmeans_model = GaussianMixture(n_components=20, random_state=42)\n","class FuzzyCMeansModel:\n","    def __init__(self, n_clusters=6, m=2, error=0.005, maxiter=1000):\n","        self.n_clusters = n_clusters\n","        self.m = m\n","        self.error = error\n","        self.maxiter = maxiter\n","        self.labels_ = None\n","        self.centers = None\n","        self.u = None  # Membership matrix\n","\n","    def fit(self, X):\n","        self.centers, self.u, _, _, _, _, _ = cmeans(\n","            X.T, self.n_clusters, self.m, error=self.error, maxiter=self.maxiter\n","        )\n","        self.labels_ = np.argmax(self.u, axis=0)\n","        return self\n","\n","    def predict(self, X):\n","        return self.labels_"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NWKeJ49nsJm7"},"outputs":[],"source":["## bertopic.py ##\n","\n","# Filter out rows where the 'embedding' is None\n","df_embeddings = df.dropna(subset=['embedding']).copy()\n","\n","# Normalize embeddings and store them in a new column\n","df_embeddings['embedding_normalized'] = df_embeddings['embedding'].apply(\n","  lambda x: normalize([x], norm='l2')[0]\n",")\n","embeddings_array = np.array(df_embeddings['embedding_normalized'].tolist())\n","\n","# Use the cleaned text for docs\n","docs = df_embeddings['flt_tokens'].tolist()\n","\n","# Initialize BERTopic model with a random_state for reproducibility\n","\n","cluster_model = FuzzyCMeansModel(n_clusters= 6)\n","bertopic_model = BERTopic(hdbscan_model=cluster_model)\n","\n","# Fit model\n","topics, probs = bertopic_model.fit_transform(docs, embeddings_array)\n","\n","# Get actual fuzzy topic probabilities from the Fuzzy C-Means model\n","membership_probs = cluster_model.u.T  # shape = (n_docs, n_topics)\n","\n","# Turn it into a DataFrame\n","membership_df = pd.DataFrame(membership_probs, columns=[f\"Topic_{i}\" for i in range(cluster_model.n_clusters)])\n","\n","# Join with df_embeddings\n","df_embeddings = df_embeddings.reset_index(drop=True)\n","df_embeddings = pd.concat([df_embeddings, membership_df], axis=1)\n","\n","# Now you can access per-topic probabilities per document\n","df_embeddings.head()\n","\n","df_embeddings[\"topic\"] = membership_df.idxmax(axis=1).apply(lambda x: int(x.split(\"_\")[1]))\n","df_embeddings[\"probs\"] = membership_df.max(axis=1)\n","\n","\n","# Print the topics and their top words after fitting the model\n","print(\"BERTopic Topics and Top Words:\")\n","print(bertopic_model.get_topics())\n","\n","# Get the current datetime for the filename\n","formatted_datetime = datetime.now().strftime(\"%d_%b_%Y_%H_%M_%S\")\n","\n","# Create the output directory for embeddings if it doesn't exist\n","output_embeddings_dir = \"../data/embeddings\"\n","os.makedirs(output_embeddings_dir, exist_ok=True)\n","\n","# Save dataframe with topics and probabilities\n","df_embeddings.to_csv(\n","  f\"{output_embeddings_dir}/feedback_embeddings_bertopic_{formatted_datetime}.csv\",\n","  index=False\n",")\n","\n","# Create the output directory for models if it doesn't exist\n","output_models_dir = \"../models\"\n","os.makedirs(output_models_dir, exist_ok=True)\n","\n","# save the model for future use on unseen data\n","joblib.dump(\n","  bertopic_model,\n","  f\"{output_models_dir}/bertopic_model_{formatted_datetime}.joblib\"\n",")\n","\n","print(\"BERTopic model worked.\")"]},{"cell_type":"code","source":["membership_df"],"metadata":{"id":"Ui5KAAfyuXtL"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H7EISzn7-gEy"},"outputs":[],"source":["df_embeddings['probs']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_aGXDv7yvNjj"},"outputs":[],"source":["number_of_topics = df_embeddings['topic'].nunique()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9twB6a6dxGNW"},"outputs":[],"source":["print(bertopic_model.get_topic_info())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9iHWliAFxS_l"},"outputs":[],"source":["# Get the topic information DataFrame\n","topic_info_df = bertopic_model.get_topic_info()\n","\n","#print(topic_info_df['Representation'])\n","# Iterate through the 'Representation' column and print each list of words\n","for index, representation in topic_info_df['Representation'].items():\n","    topic_id = topic_info_df.loc[index, 'Topic'] # Get the corresponding Topic ID\n","    print(f\"Topic {topic_id} Representation: {representation}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"Z2r2RDJOyHPJ"},"outputs":[],"source":["bertopic_model.visualize_hierarchy()"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"rOQAZLO3yLEH"},"outputs":[],"source":["hierarchical_topics = bertopic_model.hierarchical_topics(docs)\n","bertopic_model.visualize_hierarchy(hierarchical_topics=hierarchical_topics)"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"QuXVjpENyQDS"},"outputs":[],"source":["bertopic_model.visualize_barchart(top_n_topics=20, n_words=8, height=400, width=600)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HM7V9Gf2t5zy"},"outputs":[],"source":["## bertopic.py ##\n","\n","# Returns { topic_number: List[str] , ...}\n","rep_docs = bertopic_model.get_representative_docs()\n","\n","rep_docs_df = pd.DataFrame.from_dict(rep_docs)\n","rep_docs_df.to_csv(\n","  f\"../data/embeddings/representative_docs_{formatted_datetime}.csv\",\n","  index=False\n",")"]},{"cell_type":"markdown","metadata":{"id":"ew0bO5Gvyb0I"},"source":["# Visualizations"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E7OpRviDzQ5Z"},"outputs":[],"source":["model_name = \"LLM-BERTopic\""]},{"cell_type":"markdown","metadata":{"id":"5fHNkQrbydl6"},"source":["## Create Dataframe with Results"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0b82e85b"},"outputs":[],"source":["import re\n","import pandas as pd\n","\n","def format_bertopic_sentences(model, embeddings, n_topics):\n","    rows = []\n","\n","    for index, row in embeddings.iterrows():\n","        doc_num = index\n","        dominant_topic = row['topic']\n","        topic_prob = row['probs']\n","        text = row['flt_tokens']\n","\n","        # Topic contribution breakdown\n","        topic_dist = {f\"Topic_{i}_Perc\": 0.0 for i in range(n_topics)}\n","        topic_binary = {f\"Topic_{i}_Indicate\": 0 for i in range(n_topics)}\n","\n","        shared_perc = 0.0\n","        dom_topic_num = dominant_topic if dominant_topic != -1 else None\n","        dom_topic_prob = topic_prob\n","        is_outlier = (dominant_topic == -1)\n","        itr = 0\n","\n","        # Simulate topic breakdown via probability — if available\n","        if not is_outlier and isinstance(topic_prob, float):\n","            # Use the membership probabilities from the row if available\n","            for i in range(n_topics):\n","                topic_dist[f\"Topic_{i}_Perc\"] = round(row[f'Topic_{i}'] * 100, 2)\n","                if row[f'Topic_{i}'] > 0: # Indicate if there's any contribution\n","                    topic_binary[f\"Topic_{i}_Indicate\"] = 1\n","\n","            shared_perc = topic_dist[f\"Topic_{dominant_topic}_Perc\"]\n","            dom_topic_num = dominant_topic\n","            itr = sum(list(topic_binary.values())) # Count number of topics with contribution\n","        else:\n","            # Optionally apply fallback logic for topic proportion\n","            topic_dist[f\"Topic_{dominant_topic}_Perc\"] = 100.0\n","            topic_binary[f\"Topic_{dominant_topic}_Indicate\"] = 1\n","            dom_topic_num = dominant_topic\n","            itr = 1\n","\n","        # Keywords\n","        if dom_topic_num is not None and dom_topic_num != -1:\n","            keywords = \", \".join([word for word, _ in model.get_topic(dom_topic_num)])\n","        else:\n","            keywords = \"Outlier Topic\"\n","\n","        # Construct the document row\n","        row_data = {\n","            'Document_Num': doc_num,\n","            'Dominant_Topic': dom_topic_num,\n","            'Topic_%_Contrib': dom_topic_prob,\n","            'Topic_Keywords': keywords,\n","            'Text': text,\n","            'Is_Outlier': is_outlier,\n","            'Dom_Topics_Num': itr\n","        }\n","        row_data.update(topic_dist)\n","        row_data.update(topic_binary)\n","        rows.append(row_data)\n","\n","    # Create DataFrame\n","    sent_topics_df = pd.DataFrame(rows)\n","    return sent_topics_df"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZiUa1GptIAEC"},"outputs":[],"source":["dominant_topic_df = format_bertopic_sentences(model=bertopic_model, embeddings=df_embeddings, n_topics=number_of_topics)\n","dominant_topic_df"]},{"cell_type":"markdown","metadata":{"id":"t2MU1Gevyl0o"},"source":["## Data Exploration"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qsLOXGax7JDF","collapsed":true},"outputs":[],"source":["dominant_topic_df[dominant_topic_df['Dominant_Topic'] == 9]"]},{"cell_type":"markdown","metadata":{"id":"Oz-0j75qzyjK"},"source":["## Wordcloud"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"usIJunJQ033-"},"outputs":[],"source":["# Input: Receives dataframe containing all rows with classification, table title, and number of topics\n","# Output: Displays n word clouds\n","def display_wordcloud(df, title, n_topics):\n","\n","  n_cols = 2\n","  n_rows = int(np.ceil(n_topics / n_cols))\n","  fig = plt.figure(figsize=(14, 4 * n_rows))\n","\n","  for topic in range(n_topics):\n","    topic_df = dominant_topic_df[dominant_topic_df['Dominant_Topic'] == topic]\n","    text = ' '.join(topic_df['Text'].astype(str).tolist()) # Joining the words of each text of each row into a list separated by ' '\n","    text = re.sub(r'[^A-Za-z\\s]', '', text) # Substituting/deleting anything that IS NOT a letter Aa-Zz with ''\n","    text = text.lower()\n","    # Fix: Convert words_to_filter set to a list before concatenation\n","    en_stop = set(stopwords.words('english') + list(words_to_filter))\n","    text = ' '.join(word for word in text.split() if word not in en_stop)\n","\n","    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)\n","\n","    ax = fig.add_subplot(n_rows, n_cols, topic + 1)\n","    ax.imshow(wordcloud, interpolation='bilinear')\n","    ax.set_title(f\"Topic #{topic}\")\n","    ax.axis(\"off\")\n","\n","  fig.suptitle(title, fontsize=32)\n","  plt.tight_layout(rect=[0, 0, 1, 0.95])\n","  plt.show()\n","\n","\n","display_wordcloud(df=dominant_topic_df, title='Topic Classification Wordclouds', n_topics=6)"]},{"cell_type":"markdown","metadata":{"id":"F9Vcz5Gd5EZm"},"source":["## Dominant Topic Frequency"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SbVGx95esqiI"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import numpy as np # Import numpy if not already imported\n","\n","# Ensure dominant_topic_df is available\n","if  'dominant_topic_df' not in locals():\n","    print(\"Error:  dominant_topic_df' not found. Please run the cell that creates dominant_topic_df first.\")\n","else:\n","    # Calculate topic share (frequency) from dominant_topic_df\n","    df_topic_share = dominant_topic_df.groupby(['Dominant_Topic'])['Document_Num'].count().reset_index()\n","    df_topic_share.columns = ['Topic', 'Frequency'] # Rename columns for clarity\n","\n","    # Sort by frequency to potentially make the plot more informative\n","    df_topic_share = df_topic_share.sort_values('Frequency', ascending=False)\n","\n","\n","    # Define colors (optional, adjust as needed based on number of topics)\n","    # Ensure the number of colors matches the number of topics to display\n","    num_topics_to_plot = len(df_topic_share)\n","    # You can use a colormap to generate enough colors\n","    colors = plt.cm.viridis(np.linspace(0, 1, num_topics_to_plot))\n","\n","\n","    # Create the bar plot\n","    fig, ax = plt.subplots(figsize=(10, 6)) # Adjust figure size as needed\n","\n","    # Plot the bars\n","    bars = ax.bar(df_topic_share['Topic'].astype(str), df_topic_share['Frequency'], color=colors)\n","\n","    # Add labels to the bars\n","    for bar in bars:\n","        yval = bar.get_height()\n","        ax.text(bar.get_x() + bar.get_width()/2.0, yval, int(yval), va='bottom', ha='center') # va: vertical alignment, ha: horizontal alignment\n","\n","    # Set plot title and labels\n","    plt.title('Dominant Topic Frequency')\n","    plt.xlabel('Topic')\n","    plt.ylabel('Frequency')\n","    plt.xticks(rotation=45, ha='right') # Rotate x-axis labels if they overlap\n","    plt.tight_layout() # Adjust layout\n","    plt.show()"]},{"cell_type":"markdown","source":["## Topic Distribution per Doc"],"metadata":{"id":"I58dT-aunBs3"}},{"cell_type":"code","source":["def topic_dist_doc(df, n_topics, doc_num):\n","    import matplotlib.pyplot as plt\n","\n","    topics = []\n","    values = []\n","\n","    doc_row = df[df['Document_Num'] == doc_num]\n","\n","    for i in range(n_topics):\n","        col_name = f\"Topic_{i}_Perc\"\n","        val = doc_row[col_name].values[0] if not doc_row.empty and pd.notnull(doc_row[col_name].values[0]) else 0.0\n","        topics.append(col_name)\n","        values.append(float(val))  # Ensure it's a float\n","\n","    # Plotting\n","    plt.figure(figsize=(10, 6))\n","    plt.bar(topics, values, color='red')\n","    plt.xlabel('Topics')\n","    plt.ylabel('Topic Contribution')\n","    plt.title(f'{model_name} - Topic Distribution for Document {doc_num}')\n","    plt.xticks(rotation=45)\n","    plt.tight_layout()\n","    plt.show()"],"metadata":{"id":"a6adAddaznU9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["doc_number = 3    # Change This\n","topic_dist_doc(dominant_topic_df, number_of_topics, doc_num=doc_number)"],"metadata":{"id":"Sm8jjowq2U8f"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SQ_D1w2R6X-7"},"source":["## Overall Dominant Topic % Share"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m2gDilEbyUu5"},"outputs":[],"source":["# Ensure dominant_topic_df is available\n","if 'dominant_topic_df' not in locals():\n","    print(\"Error: 'dominant_topic_df' not found. Please run the cell that creates dominant_topic_df first.\")\n","else:\n","    # Plot a histogram of the 'Topic_%_Contrib' column from dominant_topic_df\n","    dominant_topic_df['Topic_%_Contrib'].plot(kind='hist', bins=20, title='Dominant Topic % Contribution', color=\"green\")\n","    plt.gca().spines[['top', 'right',]].set_visible(False)\n","    plt.xlabel('Dominant Topic % Contribution')\n","    plt.ylabel('Frequency')\n","    plt.show() # Add plt.show() to display the plot"]},{"cell_type":"markdown","metadata":{"id":"JJjtTbzAv3-a"},"source":["# Scoring Methods"]},{"cell_type":"markdown","metadata":{"id":"3aFfZ0cMtqux"},"source":["## Topic Coherence Score"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"APHTg1zzrvHO"},"outputs":[],"source":["from gensim.models.coherencemodel import CoherenceModel\n","from gensim.corpora import Dictionary\n","\n","texts = [doc.split() for doc in df_embeddings['flt_tokens']]\n","dictionary = Dictionary(texts)\n","\n","topic_words = [[word for word, _ in bertopic_model.get_topic(i)] for i in range(len(bertopic_model.get_topics())) if i != -1]\n","\n","coherence_model = CoherenceModel(\n","    topics=topic_words,\n","    texts=texts,\n","    dictionary=dictionary,\n","    coherence='c_v'\n",")\n","\n","coherence_score = coherence_model.get_coherence()\n","print(\"Topic Coherence:\", coherence_score)\n"]},{"cell_type":"markdown","metadata":{"id":"QZ5qMHiUylz4"},"source":["## Topic Diversity"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"09ilRAaFyoE8"},"outputs":[],"source":["topic_words = [bertopic_model.get_topic(i) for i in range(len(bertopic_model.get_topics())) if i != -1]\n","top_words = [word for topic in topic_words for word, _ in topic[:10]]\n","unique_words = len(set(top_words))\n","total_words = len(top_words)\n","diversity = unique_words / total_words\n","print(\"Topic Diversity:\", diversity)"]},{"cell_type":"markdown","metadata":{"id":"p-GfCWXszXYh"},"source":["## Silhouette Score"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TPDTuNeGzXzg"},"outputs":[],"source":["from sklearn.metrics import silhouette_score\n","\n","# Use embeddings and predicted topics (excluding -1)\n","mask = df_embeddings['topic'] != -1\n","score = silhouette_score(\n","    np.vstack(df_embeddings.loc[mask, 'embedding']),\n","    df_embeddings.loc[mask, 'topic']\n",")\n","print(\"Silhouette Score:\", score)"]},{"cell_type":"markdown","metadata":{"id":"UCbCQbAE31_6"},"source":["## Davies-Bouldin Index"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Lz-gvB3a37JX"},"outputs":[],"source":["from sklearn.metrics import davies_bouldin_score\n","\n","score = davies_bouldin_score(\n","    np.vstack(df_embeddings.loc[mask, 'embedding']),\n","    df_embeddings.loc[mask, 'topic']\n",")\n","print(\"Davies-Bouldin Index:\", score)"]}],"metadata":{"colab":{"machine_shape":"hm","provenance":[{"file_id":"1Ci9xMcm_2SX_nJeNqqEElCf-PEml4LvL","timestamp":1752696297417},{"file_id":"1LJtkO2L7-0wp3HZUFQRTyYFsuQnL4VK5","timestamp":1751559426156}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}